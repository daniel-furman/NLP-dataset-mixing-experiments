{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Containing Model Evaluation Pipelines\n",
    "### ACL22 SRW confidential submission\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from math import sqrt \n",
    "from scipy.stats import norm\n",
    "from random import choices\n",
    "from scipy import stats\n",
    "import random\n",
    "from statistics import mean, stdev\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from helpers import bootstrap, get_bert_average_across_text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authorship profiling internal vars\n",
    "\n",
    "experiment_name = 'final-refined-set-paper' #folder name for saving new outputs\n",
    "\n",
    "write_model_dir = 'models/'+experiment_name\n",
    "if not os.path.isdir(write_model_dir):\n",
    "    os.mkdir(write_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(class_weight='balanced', solver='newton-cg') \n",
      "\n",
      "There are 50.0% of Female profiles in the dataset. \n",
      "\n",
      "n_samples train: 372, n_features train: 10000\n",
      "n_samples val: 50, n_features val: 10000 \n",
      "\n",
      "{'accuracy': 0.64, 'roc_auc': 0.6399999999999999, 'f1': 0.7272727272727272} \n",
      "\n",
      "[[ 8 17]\n",
      " [ 1 24]] \n",
      "\n",
      "0.730, 95.0% Bootstrap confidence interval: [0.593, 0.829] \n",
      "\n",
      "\n",
      " SVC(C=1.2, class_weight='balanced', kernel='linear') \n",
      "\n",
      "There are 50.0% of Female profiles in the dataset. \n",
      "\n",
      "n_samples train: 372, n_features train: 10000\n",
      "n_samples val: 50, n_features val: 10000 \n",
      "\n",
      "{'accuracy': 0.6, 'roc_auc': 0.6000000000000001, 'f1': 0.6969696969696971} \n",
      "\n",
      "[[ 7 18]\n",
      " [ 2 23]] \n",
      "\n",
      "0.694, 95.0% Bootstrap confidence interval: [0.540, 0.817] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. gender profiling validation mixed instagram + twitter\n",
    "\"\"\"\n",
    "\n",
    "# first do LR\n",
    "\n",
    "with open('models/final-refined-set-paper/final-profiling-LR-mix.pkl', 'rb') as f:\n",
    "    model_SVM = pickle.load(f)\n",
    "print(model_SVM, '\\n')\n",
    "\n",
    "test_df = pd.read_csv('acl22-data/intermediate-data/cleaned-data-authorship-profiling/final-refined-set/facebook_celeb_profiling.csv')\n",
    "test_df['target'][test_df['target']=='F'] = 0\n",
    "test_df['target'][test_df['target']=='M'] = 1\n",
    "test_df[['post', 'target']].to_csv('acl22-data/final-data/cleaned-data-authorship-profiling/final-refined-set/facebook_celeb_profiling_test.csv')\n",
    "\n",
    "test_df = pd.read_csv('acl22-data/final-data/cleaned-data-authorship-profiling/final-refined-set/facebook_celeb_profiling_test.csv')\n",
    "y_val = test_df['target'].to_numpy(dtype = int)\n",
    "print(f\"There are {str(len(test_df['target'][test_df['target']==0])/len(test_df['target'])*100)[0:5]}% of Female profiles in the dataset. \\n\")\n",
    "\n",
    "X = pd.read_csv('acl22-data/final-data/cleaned-data-authorship-profiling/final-refined-set/mixed_celeb_profiling_folded.csv')\n",
    "X = X[[\"post\",\"target\"]]\n",
    "feature_label = 'post'\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word', \n",
    "    sublinear_tf=True, \n",
    "    strip_accents='unicode',\n",
    "    ngram_range=(2, 2),\n",
    "    max_features=10000)\n",
    "\n",
    "X_train,  y_train = [\n",
    "        X[feature_label], \n",
    "        X[\"target\"] \n",
    "      ]\n",
    "    #print(y_val)\n",
    "    # redefine Xs with tfidf\n",
    "X_train_tf = vectorizer.fit_transform(X_train)\n",
    "X_train_tf = vectorizer.transform(X_train)\n",
    "print(\"n_samples train: %d, n_features train: %d\" % X_train_tf.shape)\n",
    "    \n",
    "X_test_tf = vectorizer.transform(test_df[\"post\"])\n",
    "print(\"n_samples val: %d, n_features val: %d\" % X_test_tf.shape, '\\n')\n",
    "y_pred = model_SVM.predict(X_test_tf)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "roc_auc = roc_auc_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "    \n",
    "\n",
    "results = {'accuracy':accuracy, 'roc_auc':roc_auc, 'f1':f1}\n",
    "print(results, '\\n')\n",
    "print(confusion_matrix(y_val, y_pred), '\\n')\n",
    "\n",
    "confidence_level=0.95\n",
    "lower, median,upper,f1s=bootstrap(y_val, y_pred, B=1000,confidence_level=confidence_level)\n",
    "print(\"%.3f, %s%% Bootstrap confidence interval: [%.3f, %.3f]\" % (median, confidence_level*100, lower, upper), '\\n')\n",
    "pd.DataFrame(f1s).T.to_csv('models/final-refined-set-paper/eval_dfs/f1s_profiling_LR_mix.csv')\n",
    "\n",
    "# second do SVM\n",
    "\n",
    "with open('models/final-refined-set-paper/final-profiling-SVM-mix.pkl', 'rb') as f:\n",
    "    model_SVM = pickle.load(f)\n",
    "print('\\n', model_SVM, '\\n')\n",
    "\n",
    "test_df = pd.read_csv('acl22-data/final-data/cleaned-data-authorship-profiling/final-refined-set/facebook_celeb_profiling_test.csv')\n",
    "y_val = test_df['target'].to_numpy(dtype = int)\n",
    "print(f\"There are {str(len(test_df['target'][test_df['target']==0])/len(test_df['target'])*100)[0:5]}% of Female profiles in the dataset. \\n\")\n",
    "\n",
    "X = pd.read_csv('acl22-data/final-data/cleaned-data-authorship-profiling/final-refined-set/mixed_celeb_profiling_folded.csv')\n",
    "X = X[[\"post\",\"target\"]]\n",
    "feature_label = 'post'\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word', \n",
    "    sublinear_tf=True, \n",
    "    strip_accents='unicode',\n",
    "    ngram_range=(2, 2),\n",
    "    max_features=10000)\n",
    "\n",
    "X_train,  y_train = [\n",
    "        X[feature_label], \n",
    "        X[\"target\"] \n",
    "      ]\n",
    "    #print(y_val)\n",
    "    # redefine Xs with tfidf\n",
    "X_train_tf = vectorizer.fit_transform(X_train)\n",
    "X_train_tf = vectorizer.transform(X_train)\n",
    "print(\"n_samples train: %d, n_features train: %d\" % X_train_tf.shape)\n",
    "    \n",
    "X_test_tf = vectorizer.transform(test_df[\"post\"])\n",
    "print(\"n_samples val: %d, n_features val: %d\" % X_test_tf.shape, '\\n')\n",
    "y_pred = model_SVM.predict(X_test_tf)\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "roc_auc = roc_auc_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "    \n",
    "\n",
    "results = {'accuracy':accuracy, 'roc_auc':roc_auc, 'f1':f1}\n",
    "print(results, '\\n')\n",
    "print(confusion_matrix(y_val, y_pred), '\\n')\n",
    "\n",
    "confidence_level=0.95\n",
    "lower, median,upper,f1s=bootstrap(y_val, y_pred, B=1000,confidence_level=confidence_level)\n",
    "print(\"%.3f, %s%% Bootstrap confidence interval: [%.3f, %.3f]\" % (median, confidence_level*100, lower, upper), '\\n')\n",
    "pd.DataFrame(f1s).T.to_csv('models/final-refined-set-paper/eval_dfs/f1s_profiling_SVM_mix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(class_weight='balanced', penalty='none', solver='saga') \n",
      "\n",
      "There are 50.0% of Female profiles in the dataset. \n",
      "\n",
      "n_samples train: 372, n_features train: 10000\n",
      "n_samples val: 50, n_features val: 10000 \n",
      "\n",
      "{'accuracy': 0.68, 'roc_auc': 0.6800000000000002, 'f1': 0.7142857142857142} \n",
      "\n",
      "[[14 11]\n",
      " [ 5 20]] \n",
      "\n",
      "0.715, 95.0% Bootstrap confidence interval: [0.549, 0.836] \n",
      "\n",
      "\n",
      " SVC(C=1, class_weight='balanced', kernel='sigmoid') \n",
      "\n",
      "There are 50.0% of Female profiles in the dataset. \n",
      "\n",
      "n_samples train: 372, n_features train: 10000\n",
      "n_samples val: 50, n_features val: 10000 \n",
      "\n",
      "{'accuracy': 0.6, 'roc_auc': 0.6, 'f1': 0.6551724137931034} \n",
      "\n",
      "[[11 14]\n",
      " [ 6 19]] \n",
      "\n",
      "0.655, 95.0% Bootstrap confidence interval: [0.500, 0.787] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2. gender profiling validation twitter only\n",
    "\"\"\"\n",
    "\n",
    "# first do LR\n",
    "\n",
    "with open('models/final-refined-set-paper/final-profiling-LR-twitter.pkl', 'rb') as f:\n",
    "    model_SVM = pickle.load(f)\n",
    "print(model_SVM, '\\n')\n",
    "\n",
    "test_df = pd.read_csv('acl22-data/final-data/cleaned-data-authorship-profiling/final-refined-set/facebook_celeb_profiling_test.csv')\n",
    "y_val = test_df['target'].to_numpy(dtype = int)\n",
    "print(f\"There are {str(len(test_df['target'][test_df['target']==0])/len(test_df['target'])*100)[0:5]}% of Female profiles in the dataset. \\n\")\n",
    "\n",
    "X = pd.read_csv('acl22-data/final-data/cleaned-data-authorship-profiling/final-refined-set/twitter_celeb_profiling_folded.csv')\n",
    "X = X[[\"post\",\"target\"]]\n",
    "feature_label = 'post'\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word', \n",
    "    sublinear_tf=True, \n",
    "    strip_accents='unicode',\n",
    "    ngram_range=(2, 2),\n",
    "    max_features=10000)\n",
    "\n",
    "X_train,  y_train = [\n",
    "        X[feature_label], \n",
    "        X[\"target\"] \n",
    "      ]\n",
    "    #print(y_val)\n",
    "    # redefine Xs with tfidf\n",
    "X_train_tf = vectorizer.fit_transform(X_train)\n",
    "X_train_tf = vectorizer.transform(X_train)\n",
    "print(\"n_samples train: %d, n_features train: %d\" % X_train_tf.shape)\n",
    "    \n",
    "X_test_tf = vectorizer.transform(test_df[\"post\"])\n",
    "print(\"n_samples val: %d, n_features val: %d\" % X_test_tf.shape, '\\n')\n",
    "y_pred = model_SVM.predict(X_test_tf)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "roc_auc = roc_auc_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "    \n",
    "\n",
    "results = {'accuracy':accuracy, 'roc_auc':roc_auc, 'f1':f1}\n",
    "print(results, '\\n')\n",
    "print(confusion_matrix(y_val, y_pred), '\\n')\n",
    "\n",
    "confidence_level=0.95\n",
    "lower, median,upper,f1s=bootstrap(y_val, y_pred, B=1000,confidence_level=confidence_level)\n",
    "print(\"%.3f, %s%% Bootstrap confidence interval: [%.3f, %.3f]\" % (median, confidence_level*100, lower, upper), '\\n')\n",
    "pd.DataFrame(f1s).T.to_csv('models/final-refined-set-paper/eval_dfs/f1s_profiling_LR_twitter.csv')\n",
    "\n",
    "# second do SVM\n",
    "\n",
    "with open('models/final-refined-set-paper/final-profiling-SVM-twitter.pkl', 'rb') as f:\n",
    "    model_SVM = pickle.load(f)\n",
    "print('\\n', model_SVM, '\\n')\n",
    "\n",
    "test_df = pd.read_csv('acl22-data/final-data/cleaned-data-authorship-profiling/final-refined-set/facebook_celeb_profiling_test.csv')\n",
    "y_val = test_df['target'].to_numpy(dtype = int)\n",
    "print(f\"There are {str(len(test_df['target'][test_df['target']==0])/len(test_df['target'])*100)[0:5]}% of Female profiles in the dataset. \\n\")\n",
    "\n",
    "X = pd.read_csv('acl22-data/final-data/cleaned-data-authorship-profiling/final-refined-set/twitter_celeb_profiling_folded.csv')\n",
    "X = X[[\"post\",\"target\"]]\n",
    "feature_label = 'post'\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word', \n",
    "    sublinear_tf=True, \n",
    "    strip_accents='unicode',\n",
    "    ngram_range=(2, 2),\n",
    "    max_features=10000)\n",
    "\n",
    "X_train,  y_train = [\n",
    "        X[feature_label], \n",
    "        X[\"target\"] \n",
    "      ]\n",
    "    #print(y_val)\n",
    "    # redefine Xs with tfidf\n",
    "X_train_tf = vectorizer.fit_transform(X_train)\n",
    "X_train_tf = vectorizer.transform(X_train)\n",
    "print(\"n_samples train: %d, n_features train: %d\" % X_train_tf.shape)\n",
    "    \n",
    "X_test_tf = vectorizer.transform(test_df[\"post\"])\n",
    "print(\"n_samples val: %d, n_features val: %d\" % X_test_tf.shape, '\\n')\n",
    "y_pred = model_SVM.predict(X_test_tf)\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "roc_auc = roc_auc_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "    \n",
    "\n",
    "results = {'accuracy':accuracy, 'roc_auc':roc_auc, 'f1':f1}\n",
    "print(results, '\\n')\n",
    "print(confusion_matrix(y_val, y_pred), '\\n')\n",
    "\n",
    "confidence_level=0.95\n",
    "lower, median,upper,f1s=bootstrap(y_val, y_pred, B=1000,confidence_level=confidence_level)\n",
    "print(\"%.3f, %s%% Bootstrap confidence interval: [%.3f, %.3f]\" % (median, confidence_level*100, lower, upper), '\\n')\n",
    "pd.DataFrame(f1s).T.to_csv('models/final-refined-set-paper/eval_dfs/f1s_profiling_SVM_twitter.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(class_weight='balanced', solver='liblinear') \n",
      "\n",
      "n_samples train: 372, n_features train: 3737\n",
      "n_samples val: 60, n_features val: 3737 \n",
      "\n",
      "{'accuracy': 0.65, 'f1': 0.65} \n",
      "\n",
      "[[ 7  9  4]\n",
      " [ 0 15  5]\n",
      " [ 0  3 17]] \n",
      "\n",
      "0.650, 95.0% Bootstrap confidence interval: [0.533, 0.767] \n",
      "\n",
      "SVC(C=0.7, class_weight='balanced', kernel='linear') \n",
      "\n",
      "n_samples train: 372, n_features train: 3737\n",
      "n_samples val: 60, n_features val: 3737 \n",
      "\n",
      "{'accuracy': 0.75, 'f1': 0.75} \n",
      "\n",
      "[[11  6  3]\n",
      " [ 0 16  4]\n",
      " [ 0  2 18]] \n",
      "\n",
      "0.750, 95.0% Bootstrap confidence interval: [0.650, 0.850] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3. authorship attribution validation mixed instagram + twitter\n",
    "\"\"\"\n",
    "\n",
    "# first do LR\n",
    "\n",
    "with open('models/final-refined-set-paper/final-attribution-LR-mix.pkl', 'rb') as f:\n",
    "    model_SVM = pickle.load(f)\n",
    "print(model_SVM, '\\n')\n",
    "\n",
    "test_df = pd.read_csv('acl22-data/intermediate-data/cleaned-data-authorship-attribution/final-refined-set/facebook_celeb_attribution.csv', index_col = 'Unnamed: 0')\n",
    "test_df[['post', 'target']].to_csv('acl22-data/final-data/cleaned-data-authorship-attribution/final-refined-set/facebook_celeb_attribution_test.csv')\n",
    "\n",
    "test_df = pd.read_csv('acl22-data/final-data/cleaned-data-authorship-attribution/final-refined-set/facebook_celeb_attribution_test.csv')\n",
    "y_val = test_df['target'].to_numpy(dtype = int)\n",
    "\n",
    "X = pd.read_csv('acl22-data/final-data/cleaned-data-authorship-attribution/final-refined-set/mixed_celeb_attribution_folded.csv')\n",
    "X = X[[\"post\",\"target\"]]\n",
    "feature_label = 'post'\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word', \n",
    "    sublinear_tf=True, \n",
    "    strip_accents='unicode',\n",
    "    ngram_range=(2, 2),\n",
    "    max_features=10000)\n",
    "\n",
    "X_train,  y_train = [\n",
    "        X[feature_label], \n",
    "        X[\"target\"] \n",
    "      ]\n",
    "    #print(y_val)\n",
    "    # redefine Xs with tfidf\n",
    "X_train_tf = vectorizer.fit_transform(X_train)\n",
    "X_train_tf = vectorizer.transform(X_train)\n",
    "print(\"n_samples train: %d, n_features train: %d\" % X_train_tf.shape)\n",
    "    \n",
    "X_test_tf = vectorizer.transform(test_df[\"post\"])\n",
    "print(\"n_samples val: %d, n_features val: %d\" % X_test_tf.shape, '\\n')\n",
    "\n",
    "X_test_tf_nonsparse = pd.DataFrame.sparse.from_spmatrix(X_test_tf)\n",
    "    \n",
    "bert_feats_list = []\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "for i in range(0,len(X_test_tf_nonsparse)):\n",
    "    bert_feats_list.append(pd.DataFrame(get_bert_average_across_text_tokens(string = test_df['post'].loc[i],\n",
    "                                                                            tokenizer = BertTokenizer.from_pretrained('bert-base-cased'),\n",
    "                                                                            model_bert = BertModel.from_pretrained('bert-base-cased'))).T)\n",
    "bert_df = pd.concat(bert_feats_list)    \n",
    "    \n",
    "X_test_both = pd.merge(X_test_tf_nonsparse, bert_df, left_index=True, right_index=True, suffixes=('_tfidf', '_bert'))\n",
    "\n",
    "y_pred = model_SVM.predict(X_test_both)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred, average='micro')\n",
    "\n",
    "results = {'accuracy':accuracy, 'f1':f1}\n",
    "print(results, '\\n')\n",
    "print(confusion_matrix(y_val, y_pred), '\\n')\n",
    "\n",
    "confidence_level=0.95\n",
    "lower, median,upper,f1s=bootstrap(y_val, y_pred, B=1000,confidence_level=confidence_level, multiclass=True)\n",
    "print(\"%.3f, %s%% Bootstrap confidence interval: [%.3f, %.3f]\" % (median, confidence_level*100, lower, upper), '\\n')\n",
    "pd.DataFrame(f1s).T.to_csv('models/final-refined-set-paper/eval_dfs/f1s_attribution_LR_mix.csv')\n",
    "\n",
    "\n",
    "# second do SVM\n",
    "\n",
    "with open('models/final-refined-set-paper/final-attribution-SVM-mix.pkl', 'rb') as f:\n",
    "    model_SVM = pickle.load(f)\n",
    "print(model_SVM, '\\n')\n",
    "\n",
    "#test_df = pd.read_csv('acl22-data/intermediate-data/cleaned-data-authorship-attribution/final-refined-set/facebook_celeb_attribution.csv', index_col = 'Unnamed: 0')\n",
    "#test_df[['post', 'target']].to_csv('acl22-data/final-data/cleaned-data-authorship-attribution/final-refined-set/facebook_celeb_attribution_test.csv')\n",
    "\n",
    "test_df = pd.read_csv('acl22-data/final-data/cleaned-data-authorship-attribution/final-refined-set/facebook_celeb_attribution_test.csv')\n",
    "y_val = test_df['target'].to_numpy(dtype = int)\n",
    "\n",
    "X = pd.read_csv('acl22-data/final-data/cleaned-data-authorship-attribution/final-refined-set/mixed_celeb_attribution_folded.csv')\n",
    "X = X[[\"post\",\"target\"]]\n",
    "feature_label = 'post'\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word', \n",
    "    sublinear_tf=True, \n",
    "    strip_accents='unicode',\n",
    "    ngram_range=(2, 2),\n",
    "    max_features=10000)\n",
    "\n",
    "X_train,  y_train = [\n",
    "        X[feature_label], \n",
    "        X[\"target\"] \n",
    "      ]\n",
    "    #print(y_val)\n",
    "    # redefine Xs with tfidf\n",
    "X_train_tf = vectorizer.fit_transform(X_train)\n",
    "X_train_tf = vectorizer.transform(X_train)\n",
    "print(\"n_samples train: %d, n_features train: %d\" % X_train_tf.shape)\n",
    "    \n",
    "X_test_tf = vectorizer.transform(test_df[\"post\"])\n",
    "print(\"n_samples val: %d, n_features val: %d\" % X_test_tf.shape, '\\n')\n",
    "\n",
    "X_test_tf_nonsparse = pd.DataFrame.sparse.from_spmatrix(X_test_tf)\n",
    "    \n",
    "bert_feats_list = []\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "for i in range(0,len(X_test_tf_nonsparse)):\n",
    "    bert_feats_list.append(pd.DataFrame(get_bert_average_across_text_tokens(string = test_df['post'].loc[i],\n",
    "                                                                            tokenizer = BertTokenizer.from_pretrained('bert-base-cased'),\n",
    "                                                                            model_bert = BertModel.from_pretrained('bert-base-cased'))).T)\n",
    "bert_df = pd.concat(bert_feats_list)    \n",
    "    \n",
    "X_test_both = pd.merge(X_test_tf_nonsparse, bert_df, left_index=True, right_index=True, suffixes=('_tfidf', '_bert'))\n",
    "\n",
    "y_pred = model_SVM.predict(X_test_both)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred, average='micro')\n",
    "\n",
    "results = {'accuracy':accuracy, 'f1':f1}\n",
    "print(results, '\\n')\n",
    "print(confusion_matrix(y_val, y_pred), '\\n')\n",
    "\n",
    "confidence_level=0.95\n",
    "lower, median,upper,f1s=bootstrap(y_val, y_pred, B=1000,confidence_level=confidence_level, multiclass=True)\n",
    "print(\"%.3f, %s%% Bootstrap confidence interval: [%.3f, %.3f]\" % (median, confidence_level*100, lower, upper), '\\n')\n",
    "pd.DataFrame(f1s).T.to_csv('models/final-refined-set-paper/eval_dfs/f1s_attribution_SVM_mix.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(class_weight='balanced', solver='newton-cg') \n",
      "\n",
      "n_samples train: 372, n_features train: 3193\n",
      "n_samples val: 60, n_features val: 3193 \n",
      "\n",
      "{'accuracy': 0.6, 'f1': 0.6} \n",
      "\n",
      "[[ 6  8  6]\n",
      " [ 1 16  3]\n",
      " [ 0  6 14]] \n",
      "\n",
      "0.600, 95.0% Bootstrap confidence interval: [0.483, 0.717] \n",
      "\n",
      "SVC(C=0.6, class_weight='balanced', kernel='linear') \n",
      "\n",
      "n_samples train: 372, n_features train: 3193\n",
      "n_samples val: 60, n_features val: 3193 \n",
      "\n",
      "{'accuracy': 0.6166666666666667, 'f1': 0.6166666666666667} \n",
      "\n",
      "[[ 8  5  7]\n",
      " [ 3 14  3]\n",
      " [ 0  5 15]] \n",
      "\n",
      "0.617, 95.0% Bootstrap confidence interval: [0.500, 0.733] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "4. authorship attribution validation twitter only\n",
    "\"\"\"\n",
    "\n",
    "# first do LR\n",
    "\n",
    "with open('models/final-refined-set-paper/final-attribution-LR-twitter.pkl', 'rb') as f:\n",
    "    model_SVM = pickle.load(f)\n",
    "print(model_SVM, '\\n')\n",
    "\n",
    "test_df = pd.read_csv('acl22-data/intermediate-data/cleaned-data-authorship-attribution/final-refined-set/facebook_celeb_attribution.csv', index_col = 'Unnamed: 0')\n",
    "test_df[['post', 'target']].to_csv('acl22-data/final-data/cleaned-data-authorship-attribution/final-refined-set/facebook_celeb_attribution_test.csv')\n",
    "\n",
    "test_df = pd.read_csv('acl22-data/final-data/cleaned-data-authorship-attribution/final-refined-set/facebook_celeb_attribution_test.csv')\n",
    "y_val = test_df['target'].to_numpy(dtype = int)\n",
    "\n",
    "X = pd.read_csv('acl22-data/final-data/cleaned-data-authorship-attribution/final-refined-set/twitter_celeb_attribution_folded.csv')\n",
    "X = X[[\"post\",\"target\"]]\n",
    "feature_label = 'post'\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word', \n",
    "    sublinear_tf=True, \n",
    "    strip_accents='unicode',\n",
    "    ngram_range=(2, 2),\n",
    "    max_features=10000)\n",
    "\n",
    "X_train,  y_train = [\n",
    "        X[feature_label], \n",
    "        X[\"target\"] \n",
    "      ]\n",
    "    #print(y_val)\n",
    "    # redefine Xs with tfidf\n",
    "X_train_tf = vectorizer.fit_transform(X_train)\n",
    "X_train_tf = vectorizer.transform(X_train)\n",
    "print(\"n_samples train: %d, n_features train: %d\" % X_train_tf.shape)\n",
    "    \n",
    "X_test_tf = vectorizer.transform(test_df[\"post\"])\n",
    "print(\"n_samples val: %d, n_features val: %d\" % X_test_tf.shape, '\\n')\n",
    "\n",
    "X_test_tf_nonsparse = pd.DataFrame.sparse.from_spmatrix(X_test_tf)\n",
    "    \n",
    "bert_feats_list = []\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "for i in range(0,len(X_test_tf_nonsparse)):\n",
    "    bert_feats_list.append(pd.DataFrame(get_bert_average_across_text_tokens(string = test_df['post'].loc[i],\n",
    "                                                                            tokenizer = BertTokenizer.from_pretrained('bert-base-cased'),\n",
    "                                                                            model_bert = BertModel.from_pretrained('bert-base-cased'))).T)\n",
    "bert_df = pd.concat(bert_feats_list)    \n",
    "    \n",
    "X_test_both = pd.merge(X_test_tf_nonsparse, bert_df, left_index=True, right_index=True, suffixes=('_tfidf', '_bert'))\n",
    "\n",
    "y_pred = model_SVM.predict(X_test_both)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred, average='micro')\n",
    "\n",
    "results = {'accuracy':accuracy, 'f1':f1}\n",
    "print(results, '\\n')\n",
    "print(confusion_matrix(y_val, y_pred), '\\n')\n",
    "\n",
    "confidence_level=0.95\n",
    "lower, median,upper,f1s=bootstrap(y_val, y_pred, B=1000,confidence_level=confidence_level, multiclass=True)\n",
    "print(\"%.3f, %s%% Bootstrap confidence interval: [%.3f, %.3f]\" % (median, confidence_level*100, lower, upper), '\\n')\n",
    "pd.DataFrame(f1s).T.to_csv('models/final-refined-set-paper/eval_dfs/f1s_attribution_LR_twitter.csv')\n",
    "\n",
    "\n",
    "# second do SVM\n",
    "\n",
    "with open('models/final-refined-set-paper/final-attribution-SVM-twitter.pkl', 'rb') as f:\n",
    "    model_SVM = pickle.load(f)\n",
    "print(model_SVM, '\\n')\n",
    "\n",
    "#test_df = pd.read_csv('acl22-data/intermediate-data/cleaned-data-authorship-attribution/final-refined-set/facebook_celeb_attribution.csv', index_col = 'Unnamed: 0')\n",
    "#test_df[['post', 'target']].to_csv('acl22-data/final-data/cleaned-data-authorship-attribution/final-refined-set/facebook_celeb_attribution_test.csv')\n",
    "\n",
    "test_df = pd.read_csv('acl22-data/final-data/cleaned-data-authorship-attribution/final-refined-set/facebook_celeb_attribution_test.csv')\n",
    "y_val = test_df['target'].to_numpy(dtype = int)\n",
    "\n",
    "X = pd.read_csv('acl22-data/final-data/cleaned-data-authorship-attribution/final-refined-set/twitter_celeb_attribution_folded.csv')\n",
    "X = X[[\"post\",\"target\"]]\n",
    "feature_label = 'post'\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word', \n",
    "    sublinear_tf=True, \n",
    "    strip_accents='unicode',\n",
    "    ngram_range=(2, 2),\n",
    "    max_features=10000)\n",
    "\n",
    "X_train,  y_train = [\n",
    "        X[feature_label], \n",
    "        X[\"target\"] \n",
    "      ]\n",
    "    #print(y_val)\n",
    "    # redefine Xs with tfidf\n",
    "X_train_tf = vectorizer.fit_transform(X_train)\n",
    "X_train_tf = vectorizer.transform(X_train)\n",
    "print(\"n_samples train: %d, n_features train: %d\" % X_train_tf.shape)\n",
    "    \n",
    "X_test_tf = vectorizer.transform(test_df[\"post\"])\n",
    "print(\"n_samples val: %d, n_features val: %d\" % X_test_tf.shape, '\\n')\n",
    "\n",
    "X_test_tf_nonsparse = pd.DataFrame.sparse.from_spmatrix(X_test_tf)\n",
    "    \n",
    "bert_feats_list = []\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "for i in range(0,len(X_test_tf_nonsparse)):\n",
    "    bert_feats_list.append(pd.DataFrame(get_bert_average_across_text_tokens(string = test_df['post'].loc[i],\n",
    "                                                                            tokenizer = BertTokenizer.from_pretrained('bert-base-cased'),\n",
    "                                                                            model_bert = BertModel.from_pretrained('bert-base-cased'))).T)\n",
    "bert_df = pd.concat(bert_feats_list)    \n",
    "    \n",
    "X_test_both = pd.merge(X_test_tf_nonsparse, bert_df, left_index=True, right_index=True, suffixes=('_tfidf', '_bert'))\n",
    "\n",
    "y_pred = model_SVM.predict(X_test_both)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred, average='micro')\n",
    "\n",
    "results = {'accuracy':accuracy, 'f1':f1}\n",
    "print(results, '\\n')\n",
    "print(confusion_matrix(y_val, y_pred), '\\n')\n",
    "\n",
    "confidence_level=0.95\n",
    "lower, median,upper,f1s=bootstrap(y_val, y_pred, B=1000,confidence_level=confidence_level, multiclass=True)\n",
    "print(\"%.3f, %s%% Bootstrap confidence interval: [%.3f, %.3f]\" % (median, confidence_level*100, lower, upper), '\\n')\n",
    "pd.DataFrame(f1s).T.to_csv('models/final-refined-set-paper/eval_dfs/f1s_attribution_SVM_twitter.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mix vs. twitter authorship attribution SVM: \n",
      "      Bonferonni corrected P_val for Welch's T-test: 0.0, with a 95% confidence interval of [0.0,0.0]\n",
      "\n",
      "      The Cohens d effect size is 2.2621726227507217\n",
      "Mix vs. twitter authorship attribution LR: \n",
      "      Bonferonni corrected P_val for Welch's T-test: 1.9881530707118724e-71, with a 95% confidence interval of [1.74065854739331e-85,1.1882912739943515e-57]\n",
      "\n",
      "      The Cohens d effect size is 0.8307029048234137\n",
      "Mix vs. twitter authorship profiling SVM: \n",
      "      Bonferonni corrected P_val for Welch's T-test: 7.07880319054289e-30, with a 95% confidence interval of [5.973494804257549e-40,3.7500356952609115e-21]\n",
      "\n",
      "      The Cohens d effect size is 0.5182348084916546\n",
      "Mix vs. twitter authorship profiling LR: \n",
      "      Bonferonni corrected P_val for Welch's T-test: 2.1030858084979975e-07, with a 95% confidence interval of [2.2564229801414682e-12,0.0012105692679097546]\n",
      "\n",
      "      The Cohens d effect size is 0.238442055551052\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5. hypothesis testing and practical significance\n",
    "\n",
    "One sided Welch's T-test, which doesn't assume equal variances. Using bootstrap to generate confidence intervals.\n",
    "Null is that the difference is zero\n",
    "Alternative is that multi-platform modeling is superior\n",
    "\n",
    "Cohen's d employed for effect size.\n",
    "\"\"\"\n",
    "\n",
    "# mix vs. twitter authorship attribution SVM\n",
    "\n",
    "f1s_AA_SVM_Twitter =pd.read_csv('models/final-refined-set-paper/eval_dfs/f1s_attribution_SVM_twitter.csv', index_col = 'Unnamed: 0')\n",
    "f1s_AA_SVM_mix = pd.read_csv('models/final-refined-set-paper/eval_dfs/f1s_attribution_SVM_mix.csv', index_col = 'Unnamed: 0')\n",
    "p_val_list = []\n",
    "for i in range(0,1000):\n",
    "    f1s_AA_SVM_mix_bootstrap = f1s_AA_SVM_mix.T[0].sample(frac=1, replace=True).to_list()\n",
    "    f1s_AA_SVM_Twitter_bootstrap = f1s_AA_SVM_Twitter.T[0].sample(frac=1, replace=True).to_list()\n",
    "    p_val = stats.ttest_ind(f1s_AA_SVM_mix_bootstrap, f1s_AA_SVM_Twitter_bootstrap, equal_var=False, alternative = 'greater')[1]\n",
    "    p_val_list.append(p_val)\n",
    "p_val_list.sort()\n",
    "lower = p_val_list[25]\n",
    "median = p_val_list[500]\n",
    "upper = p_val_list[975]\n",
    "print(f\"Mix vs. twitter authorship attribution SVM: \\n      Bonferonni corrected P_val for Welch's T-test: {median*4}, with a 95% confidence interval of [{lower*4},{upper*4}]\\n\")\n",
    "c0 = f1s_AA_SVM_mix\n",
    "c0 = c0.values.tolist()[0]\n",
    "c1 = f1s_AA_SVM_Twitter\n",
    "c1 = c1.values.tolist()[0]\n",
    "cohens_d = (mean(c0) - mean(c1)) / (sqrt((stdev(c0) ** 2 + stdev(c1) ** 2) / 2))\n",
    "print('      The Cohens d effect size is', cohens_d)\n",
    "\n",
    "\n",
    "# mix vs. twitter authorship attribution LR\n",
    "\n",
    "f1s_AA_SVM_Twitter =pd.read_csv('models/final-refined-set-paper/eval_dfs/f1s_attribution_LR_twitter.csv', index_col = 'Unnamed: 0')\n",
    "f1s_AA_SVM_mix = pd.read_csv('models/final-refined-set-paper/eval_dfs/f1s_attribution_LR_mix.csv', index_col = 'Unnamed: 0')\n",
    "p_val_list = []\n",
    "for i in range(0,1000):\n",
    "    f1s_AA_SVM_mix_bootstrap = f1s_AA_SVM_mix.T[0].sample(frac=1, replace=True).to_list()\n",
    "    f1s_AA_SVM_Twitter_bootstrap = f1s_AA_SVM_Twitter.T[0].sample(frac=1, replace=True).to_list()\n",
    "    p_val = stats.ttest_ind(f1s_AA_SVM_mix_bootstrap, f1s_AA_SVM_Twitter_bootstrap, equal_var=False, alternative = 'greater')[1]\n",
    "    p_val_list.append(p_val)\n",
    "p_val_list.sort()\n",
    "lower = p_val_list[25]\n",
    "median = p_val_list[500]\n",
    "upper = p_val_list[975]\n",
    "print(f\"Mix vs. twitter authorship attribution LR: \\n      Bonferonni corrected P_val for Welch's T-test: {median*4}, with a 95% confidence interval of [{lower*4},{upper*4}]\\n\")\n",
    "c0 = f1s_AA_SVM_mix\n",
    "c0 = c0.values.tolist()[0]\n",
    "c1 = f1s_AA_SVM_Twitter\n",
    "c1 = c1.values.tolist()[0]\n",
    "cohens_d = (mean(c0) - mean(c1)) / (sqrt((stdev(c0) ** 2 + stdev(c1) ** 2) / 2))\n",
    "print('      The Cohens d effect size is', cohens_d)\n",
    "\n",
    "# mix vs. twitter gender profiling SVM\n",
    "\n",
    "f1s_AA_SVM_Twitter =pd.read_csv('models/final-refined-set-paper/eval_dfs/f1s_profiling_SVM_twitter.csv', index_col = 'Unnamed: 0')\n",
    "f1s_AA_SVM_mix = pd.read_csv('models/final-refined-set-paper/eval_dfs/f1s_profiling_SVM_mix.csv', index_col = 'Unnamed: 0')\n",
    "p_val_list = []\n",
    "for i in range(0,1000):\n",
    "    f1s_AA_SVM_mix_bootstrap = f1s_AA_SVM_mix.T[0].sample(frac=1, replace=True).to_list()\n",
    "    f1s_AA_SVM_Twitter_bootstrap = f1s_AA_SVM_Twitter.T[0].sample(frac=1, replace=True).to_list()\n",
    "    p_val = stats.ttest_ind(f1s_AA_SVM_mix_bootstrap, f1s_AA_SVM_Twitter_bootstrap, equal_var=False, alternative = 'greater')[1]\n",
    "    p_val_list.append(p_val)\n",
    "p_val_list.sort()\n",
    "lower = p_val_list[25]\n",
    "median = p_val_list[500]\n",
    "upper = p_val_list[975]\n",
    "print(f\"Mix vs. twitter authorship profiling SVM: \\n      Bonferonni corrected P_val for Welch's T-test: {median*4}, with a 95% confidence interval of [{lower*4},{upper*4}]\\n\")\n",
    "c0 = f1s_AA_SVM_mix\n",
    "c0 = c0.values.tolist()[0]\n",
    "c1 = f1s_AA_SVM_Twitter\n",
    "c1 = c1.values.tolist()[0]\n",
    "cohens_d = (mean(c0) - mean(c1)) / (sqrt((stdev(c0) ** 2 + stdev(c1) ** 2) / 2))\n",
    "print('      The Cohens d effect size is', cohens_d)\n",
    "\n",
    "# mix vs. twitter gender profiling LR\n",
    "\n",
    "f1s_AA_SVM_Twitter =pd.read_csv('models/final-refined-set-paper/eval_dfs/f1s_profiling_LR_twitter.csv', index_col = 'Unnamed: 0')\n",
    "f1s_AA_SVM_mix = pd.read_csv('models/final-refined-set-paper/eval_dfs/f1s_profiling_LR_mix.csv', index_col = 'Unnamed: 0')\n",
    "p_val_list = []\n",
    "for i in range(0,1000):\n",
    "    f1s_AA_SVM_mix_bootstrap = f1s_AA_SVM_mix.T[0].sample(frac=1, replace=True).to_list()\n",
    "    f1s_AA_SVM_Twitter_bootstrap = f1s_AA_SVM_Twitter.T[0].sample(frac=1, replace=True).to_list()\n",
    "    p_val = stats.ttest_ind(f1s_AA_SVM_mix_bootstrap, f1s_AA_SVM_Twitter_bootstrap, equal_var=False, alternative = 'greater')[1]\n",
    "    p_val_list.append(p_val)\n",
    "p_val_list.sort()\n",
    "lower = p_val_list[25]\n",
    "median = p_val_list[500]\n",
    "upper = p_val_list[975]\n",
    "print(f\"Mix vs. twitter authorship profiling LR: \\n      Bonferonni corrected P_val for Welch's T-test: {median*4}, with a 95% confidence interval of [{lower*4},{upper*4}]\\n\")\n",
    "c0 = f1s_AA_SVM_mix\n",
    "c0 = c0.values.tolist()[0]\n",
    "c1 = f1s_AA_SVM_Twitter\n",
    "c1 = c1.values.tolist()[0]\n",
    "cohens_d = (mean(c0) - mean(c1)) / (sqrt((stdev(c0) ** 2 + stdev(c1) ** 2) / 2))\n",
    "print('      The Cohens d effect size is', cohens_d)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4222c62deee789526b2e6b735eb2f20b12feb76cd92f9c10ded5d55a4a9b08d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('anlp': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
