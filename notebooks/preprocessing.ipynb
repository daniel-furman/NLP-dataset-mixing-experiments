{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Containing Text Pre-Processing Pipelines\n",
    "### ACL22 SRW confidential submission\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/danielfurman/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/danielfurman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from argparse import ArgumentParser\n",
    "import importlib\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "import nltk \n",
    "nltk.download('words')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "from helpers import remove_emojis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authorship attribution internal vars\n",
    "\n",
    "experiment_name = 'v0' #folder name for saving new outputs\n",
    "chars_valid = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\"\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\",\n",
    "               \"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\n",
    "               \"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\",\"@\",\"#\",\"<\",\">\"]\n",
    "\n",
    "write_data_dir = 'acl22-data/intermediate-data/cleaned-data-authorship-attribution/'+experiment_name\n",
    "if not os.path.isdir(write_data_dir):\n",
    "    os.mkdir(write_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ['acl22-data/raw-data/raw-data-authorship-attribution/KendallJenner-raw-scraped-twitter.csv', 'acl22-data/raw-data/raw-data-authorship-attribution/KimKardashian-raw-scraped-twitter.csv', 'acl22-data/raw-data/raw-data-authorship-attribution/KylieJenner-raw-scraped-twitter.csv'] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                  0                                                  1\n",
      "151  KendallJenner  NEW @Versace CAMPAIGN BY MERT&amp;MARCUS https...\n",
      "114  KendallJenner  wearing our KENDALL X KYLIE makeup collab and ... \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                  0                                                  1\n",
      "25   KimKardashian  üíû Pinkalicious Balenciaga üíû https://t.co/mbBng...\n",
      "107  KimKardashian  The final decision will be up to Gov. Kevin St... \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "              0                                                  1\n",
      "5  KylieJenner  one hour to go!!! https://t.co/bDaiohhXCV http...\n",
      "7  KylieJenner  restocking my @kyliebaby sets, bundles and moi... \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>target</th>\n",
       "      <th>handle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>leo season loading &lt;url&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>KylieJenner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>theres a lot of eole trying to get on the site...</td>\n",
       "      <td>2</td>\n",
       "      <td>KylieJenner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>the last eisode of INSIDE KYLIE COSMETICS just...</td>\n",
       "      <td>2</td>\n",
       "      <td>KylieJenner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1 hour to go &lt;url&gt; @&lt;user&gt; 9am PST</td>\n",
       "      <td>2</td>\n",
       "      <td>KylieJenner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>KYLIE COSMETICS OFFICIALLY RELAUNCHES TOMORROW...</td>\n",
       "      <td>2</td>\n",
       "      <td>KylieJenner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 post target       handle\n",
       "57                           leo season loading <url>      2  KylieJenner\n",
       "58  theres a lot of eole trying to get on the site...      2  KylieJenner\n",
       "59  the last eisode of INSIDE KYLIE COSMETICS just...      2  KylieJenner\n",
       "60                 1 hour to go <url> @<user> 9am PST      2  KylieJenner\n",
       "61  KYLIE COSMETICS OFFICIALLY RELAUNCHES TOMORROW...      2  KylieJenner"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. authorship attribution twitter\n",
    "\"\"\"\n",
    "\n",
    "file_list = glob.glob('acl22-data/raw-data/raw-data-authorship-attribution/*-twitter.csv')\n",
    "len(file_list)\n",
    "print('\\n\\n', file_list, '\\n\\n')\n",
    "pd_df_list = []\n",
    "for f in file_list:\n",
    "    pd_df_list.append(pd.read_csv(f, header=None))\n",
    "\n",
    "print('\\n\\n', pd_df_list[0].sample(2), '\\n\\n')\n",
    "print('\\n\\n', pd_df_list[1].sample(2), '\\n\\n')\n",
    "print('\\n\\n', pd_df_list[2].sample(2), '\\n\\n')\n",
    "\n",
    "# main pipeline twitter data\n",
    "for i in range(0, len(pd_df_list)):\n",
    "\n",
    "    # if there are at least 62 lines:\n",
    "    if len(pd_df_list[i][1])>=62*2:\n",
    "        for j in range(0,61*2):\n",
    "            string=str(pd_df_list[i][1].loc[j])\n",
    "\n",
    "            # remove emojis \n",
    "            string = remove_emojis(string)\n",
    "\n",
    "            string_list = string.split(' ')\n",
    "            itr = 0\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "\n",
    "                    #replacing <user>\n",
    "                    if word[0] == \"@\":\n",
    "                        string_list[itr] = '@<user>'\n",
    "\n",
    "                    #replacing <url>\n",
    "                    elif word[0:5] == 'https':\n",
    "                        string_list[itr] = '<url>'\n",
    "            \n",
    "                itr+=1\n",
    "\n",
    "            string = \" \".join(string_list)\n",
    "\n",
    "            #replacing invalid chars\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "                    for ii in range(0, len(word)):\n",
    "                        if word[ii] not in chars_valid:\n",
    "                            string = string.replace(word[ii],'')\n",
    "                            \n",
    "            # tokenize\n",
    "            words_tok = word_tokenize(string)\n",
    "            sent = \" \".join(words_tok)\n",
    "            sent = sent.replace('@ < user >', '@<user>')\n",
    "            sent = sent.replace('< url >', '<url>')\n",
    "\n",
    "            # replacing multiple spaces\n",
    "            sent = sent.replace('  ', ' ')\n",
    "            sent = sent.replace('   ', ' ')\n",
    "            sent = sent.replace('    ', ' ')\n",
    "            sent = sent.replace('     ', ' ')\n",
    "            sent = sent.replace('      ', ' ')\n",
    "\n",
    "            # replace breaks\n",
    "            sent = sent.replace(\"break\", \"<break>\")\n",
    "            sent = sent.replace(\"BREAK\", \"<break>\")\n",
    "\n",
    "            \n",
    "            sent = sent.replace(\"\\n\", \"<break>\")\n",
    "            pd_df_list[i][1].loc[j] = sent\n",
    "        pd_df_list[i] = pd_df_list[i].loc[0:61*2] \n",
    "\n",
    "    # else there are less than 62 lines\n",
    "    else:\n",
    "        for j in range(0,len(pd_df_list[i][1])):\n",
    "            string=str(pd_df_list[i][1].loc[j])\n",
    "\n",
    "            # remove emojis \n",
    "            string = remove_emojis(string)\n",
    "\n",
    "            string_list = string.split(' ')\n",
    "            itr = 0\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "\n",
    "                    #replacing <user>\n",
    "                    if word[0] == \"@\":\n",
    "                        string_list[itr] = '@<user>'\n",
    "\n",
    "                    #replacing <url>\n",
    "                    elif word[0:5] == 'https':\n",
    "                        string_list[itr] = '<url>'\n",
    "            \n",
    "                itr+=1\n",
    "\n",
    "            string = \" \".join(string_list)\n",
    "\n",
    "            #replacing invalid chars\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "                    for ii in range(0, len(word)):\n",
    "                        if word[ii] not in chars_valid:\n",
    "                            string = string.replace(word[ii],'')\n",
    "                            \n",
    "            # tokenize\n",
    "            words_tok = word_tokenize(string)\n",
    "            sent = \" \".join(words_tok)\n",
    "            sent = sent.replace('@ < user >', '@<user>')\n",
    "            sent = sent.replace('< url >', '<url>')\n",
    "\n",
    "            # replacing multiple spaces\n",
    "            sent = sent.replace('  ', ' ')\n",
    "            sent = sent.replace('   ', ' ')\n",
    "            sent = sent.replace('    ', ' ')\n",
    "            sent = sent.replace('     ', ' ')\n",
    "            sent = sent.replace('      ', ' ')\n",
    "\n",
    "            # replace breaks\n",
    "            sent = sent.replace(\"break\", \"<break>\")\n",
    "            sent = sent.replace(\"\\n\", \"<break>\")\n",
    "            sent = sent.replace(\"BREAK\", \"<break>\")\n",
    "            \n",
    "            pd_df_list[i][1].loc[j] = sent\n",
    "\n",
    "        pd_df_list[i] = pd_df_list[i].loc[0:len(pd_df_list[i][1])]\n",
    "\n",
    "# group into one pandas dataframe\n",
    "for i in range(0, len(pd_df_list)):\n",
    "    if i == 0:\n",
    "        pd_df = pd_df_list[i]\n",
    "    else:\n",
    "        pd_df = pd.concat([pd_df, pd_df_list[i]])\n",
    "\n",
    "pd_df.columns = ['handle', 'post']\n",
    "\n",
    "pd_df['target'] = pd_df['handle']\n",
    "pd_df['target'][pd_df['handle']=='KimKardashian'] = 0\n",
    "pd_df['target'][pd_df['handle']=='KendallJenner'] = 1\n",
    "pd_df['target'][pd_df['handle']=='KylieJenner'] = 2\n",
    "\n",
    "pd_df[['post', 'target', 'handle']].to_csv(write_data_dir+'/twitter_celeb_attribution.csv')\n",
    "pd_df[['post', 'target', 'handle']].tail(5)\n",
    "\n",
    "# pipeline for mixing twitter data (half the size)\n",
    "for i in range(0, len(pd_df_list)):\n",
    "\n",
    "    # if there are at least 62 lines:\n",
    "    if len(pd_df_list[i][1])>=62:\n",
    "        for j in range(0,61):\n",
    "            string=str(pd_df_list[i][1].loc[j])\n",
    "\n",
    "            # remove emojis \n",
    "            string = remove_emojis(string)\n",
    "\n",
    "            string_list = string.split(' ')\n",
    "            itr = 0\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "\n",
    "                    #replacing <user>\n",
    "                    if word[0] == \"@\":\n",
    "                        string_list[itr] = '@<user>'\n",
    "\n",
    "                    #replacing <url>\n",
    "                    elif word[0:5] == 'https':\n",
    "                        string_list[itr] = '<url>'\n",
    "            \n",
    "                itr+=1\n",
    "\n",
    "            string = \" \".join(string_list)\n",
    "\n",
    "            #replacing invalid chars\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "                    for ii in range(0, len(word)):\n",
    "                        if word[ii] not in chars_valid:\n",
    "                            string = string.replace(word[ii],'')\n",
    "                            \n",
    "            # tokenize\n",
    "            words_tok = word_tokenize(string)\n",
    "            sent = \" \".join(words_tok)\n",
    "            sent = sent.replace('@ < user >', '@<user>')\n",
    "            sent = sent.replace('< url >', '<url>')\n",
    "\n",
    "            # replacing multiple spaces\n",
    "            sent = sent.replace('  ', ' ')\n",
    "            sent = sent.replace('   ', ' ')\n",
    "            sent = sent.replace('    ', ' ')\n",
    "            sent = sent.replace('     ', ' ')\n",
    "            sent = sent.replace('      ', ' ')\n",
    "\n",
    "            # replace breaks\n",
    "            sent = sent.replace(\"break\", \"<break>\")\n",
    "            sent = sent.replace(\"BREAK\", \"<break>\")\n",
    "\n",
    "            \n",
    "            sent = sent.replace(\"\\n\", \"<break>\")\n",
    "            pd_df_list[i][1].loc[j] = sent\n",
    "        pd_df_list[i] = pd_df_list[i].loc[0:61] \n",
    "\n",
    "    # else there are less than 62 lines\n",
    "    else:\n",
    "        for j in range(0,len(pd_df_list[i][1])):\n",
    "            string=str(pd_df_list[i][1].loc[j])\n",
    "\n",
    "            # remove emojis \n",
    "            string = remove_emojis(string)\n",
    "\n",
    "            string_list = string.split(' ')\n",
    "            itr = 0\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "\n",
    "                    #replacing <user>\n",
    "                    if word[0] == \"@\":\n",
    "                        string_list[itr] = '@<user>'\n",
    "\n",
    "                    #replacing <url>\n",
    "                    elif word[0:5] == 'https':\n",
    "                        string_list[itr] = '<url>'\n",
    "            \n",
    "                itr+=1\n",
    "\n",
    "            string = \" \".join(string_list)\n",
    "\n",
    "            #replacing invalid chars\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "                    for ii in range(0, len(word)):\n",
    "                        if word[ii] not in chars_valid:\n",
    "                            string = string.replace(word[ii],'')\n",
    "                            \n",
    "            # tokenize\n",
    "            words_tok = word_tokenize(string)\n",
    "            sent = \" \".join(words_tok)\n",
    "            sent = sent.replace('@ < user >', '@<user>')\n",
    "            sent = sent.replace('< url >', '<url>')\n",
    "\n",
    "            # replacing multiple spaces\n",
    "            sent = sent.replace('  ', ' ')\n",
    "            sent = sent.replace('   ', ' ')\n",
    "            sent = sent.replace('    ', ' ')\n",
    "            sent = sent.replace('     ', ' ')\n",
    "            sent = sent.replace('      ', ' ')\n",
    "\n",
    "            # replace breaks\n",
    "            sent = sent.replace(\"break\", \"<break>\")\n",
    "            sent = sent.replace(\"\\n\", \"<break>\")\n",
    "            sent = sent.replace(\"BREAK\", \"<break>\")\n",
    "            \n",
    "            pd_df_list[i][1].loc[j] = sent\n",
    "\n",
    "        pd_df_list[i] = pd_df_list[i].loc[0:len(pd_df_list[i][1])]\n",
    "\n",
    "# group into one pandas dataframe\n",
    "for i in range(0, len(pd_df_list)):\n",
    "    if i == 0:\n",
    "        pd_df = pd_df_list[i]\n",
    "    else:\n",
    "        pd_df = pd.concat([pd_df, pd_df_list[i]])\n",
    "\n",
    "pd_df.columns = ['handle', 'post']\n",
    "\n",
    "pd_df['target'] = pd_df['handle']\n",
    "pd_df['target'][pd_df['handle']=='KimKardashian'] = 0\n",
    "pd_df['target'][pd_df['handle']=='KendallJenner'] = 1\n",
    "pd_df['target'][pd_df['handle']=='KylieJenner'] = 2\n",
    "\n",
    "pd_df[['post', 'target', 'handle']].to_csv(write_data_dir+'/twitter_mixing_celeb_attribution.csv')\n",
    "pd_df[['post', 'target', 'handle']].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd1e015d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ['acl22-data/raw-data/raw-data-authorship-attribution/KylieJenner-raw-scraped-instagram.csv', 'acl22-data/raw-data/raw-data-authorship-attribution/KendallJenner-raw-scraped-instagram.csv', 'acl22-data/raw-data/raw-data-authorship-attribution/KimKardashian-raw-scraped-instagram.csv'] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "               0          1\n",
      "46  KylieJenner    XXIII üñ§\n",
      "20  KylieJenner  you call? \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                0                                           1\n",
      "2  KendallJenner  thank you for all the birthday wishes ‚ù§Ô∏è‚Äçüî•\n",
      "1  KendallJenner                                         ü§çü§çü§ç \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                 0                                                  1\n",
      "55  KimKardashian  My baby boy Psalm is the sweetest! He just sta...\n",
      "2   KimKardashian                                     Heart and Soul \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>target</th>\n",
       "      <th>handle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Im so excited that @&lt;user&gt; is now available at...</td>\n",
       "      <td>2</td>\n",
       "      <td>KylieJenner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>These two are besties True and Psalm</td>\n",
       "      <td>0</td>\n",
       "      <td>KimKardashian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>@&lt;user&gt; in SOHO by @&lt;user&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>KendallJenner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thank you so much to the @&lt;user&gt; for the honor...</td>\n",
       "      <td>0</td>\n",
       "      <td>KimKardashian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>FAM</td>\n",
       "      <td>0</td>\n",
       "      <td>KimKardashian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 post target         handle\n",
       "35  Im so excited that @<user> is now available at...      2    KylieJenner\n",
       "31               These two are besties True and Psalm      0  KimKardashian\n",
       "34                         @<user> in SOHO by @<user>      1  KendallJenner\n",
       "5   Thank you so much to the @<user> for the honor...      0  KimKardashian\n",
       "57                                                FAM      0  KimKardashian"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "2. authorship attribution instagram\n",
    "\"\"\"\n",
    "\n",
    "file_list = glob.glob('acl22-data/raw-data/raw-data-authorship-attribution/*-instagram.csv')\n",
    "len(file_list)\n",
    "print('\\n\\n', file_list, '\\n\\n')\n",
    "pd_df_list = []\n",
    "for f in file_list:\n",
    "    pd_df_list.append(pd.read_csv(f, header=None))\n",
    "\n",
    "print('\\n\\n', pd_df_list[0].sample(2), '\\n\\n')\n",
    "print('\\n\\n', pd_df_list[1].sample(2), '\\n\\n')\n",
    "print('\\n\\n', pd_df_list[2].sample(2), '\\n\\n')\n",
    "\n",
    "# main pipeline\n",
    "for i in range(0, len(pd_df_list)):\n",
    "\n",
    "    # if there are at least 62 lines:\n",
    "    if len(pd_df_list[i][1])>=62:\n",
    "        for j in range(0,61):\n",
    "            string=str(pd_df_list[i][1].loc[j])\n",
    "\n",
    "            # remove emojis \n",
    "            string = remove_emojis(string)\n",
    "\n",
    "            string_list = string.split(' ')\n",
    "            itr = 0\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "\n",
    "                    #replacing <user>\n",
    "                    if word[0] == \"@\":\n",
    "                        string_list[itr] = '@<user>'\n",
    "\n",
    "                    #replacing <url>\n",
    "                    elif word[0:5] == 'https':\n",
    "                        string_list[itr] = '<url>'\n",
    "            \n",
    "                itr+=1\n",
    "\n",
    "            string = \" \".join(string_list)\n",
    "\n",
    "            #replacing invalid chars\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "                    for ii in range(0, len(word)):\n",
    "                        if word[ii] not in chars_valid:\n",
    "                            string = string.replace(word[ii],'')\n",
    "                            \n",
    "            # tokenize\n",
    "            words_tok = word_tokenize(string)\n",
    "            sent = \" \".join(words_tok)\n",
    "            sent = sent.replace('@ < user >', '@<user>')\n",
    "            sent = sent.replace('< url >', '<url>')\n",
    "\n",
    "            # replacing multiple spaces\n",
    "            sent = sent.replace('  ', ' ')\n",
    "            sent = sent.replace('   ', ' ')\n",
    "            sent = sent.replace('    ', ' ')\n",
    "            sent = sent.replace('     ', ' ')\n",
    "            sent = sent.replace('      ', ' ')\n",
    "\n",
    "            # replace breaks\n",
    "            sent = sent.replace(\"break\", \"<break>\")\n",
    "            sent = sent.replace(\"BREAK\", \"<break>\")\n",
    "\n",
    "            \n",
    "            sent = sent.replace(\"\\n\", \"<break>\")\n",
    "            pd_df_list[i][1].loc[j] = sent\n",
    "        pd_df_list[i] = pd_df_list[i].loc[0:61] \n",
    "\n",
    "    # else there are less than 62 lines\n",
    "    else:\n",
    "        for j in range(0,len(pd_df_list[i][1])):\n",
    "            string=str(pd_df_list[i][1].loc[j])\n",
    "\n",
    "            # remove emojis \n",
    "            string = remove_emojis(string)\n",
    "\n",
    "            string_list = string.split(' ')\n",
    "            itr = 0\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "\n",
    "                    #replacing <user>\n",
    "                    if word[0] == \"@\":\n",
    "                        string_list[itr] = '@<user>'\n",
    "\n",
    "                    #replacing <url>\n",
    "                    elif word[0:5] == 'https':\n",
    "                        string_list[itr] = '<url>'\n",
    "            \n",
    "                itr+=1\n",
    "\n",
    "            string = \" \".join(string_list)\n",
    "\n",
    "            #replacing invalid chars\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "                    for ii in range(0, len(word)):\n",
    "                        if word[ii] not in chars_valid:\n",
    "                            string = string.replace(word[ii],'')\n",
    "                            \n",
    "            # tokenize\n",
    "            words_tok = word_tokenize(string)\n",
    "            sent = \" \".join(words_tok)\n",
    "            sent = sent.replace('@ < user >', '@<user>')\n",
    "            sent = sent.replace('< url >', '<url>')\n",
    "\n",
    "            # replacing multiple spaces\n",
    "            sent = sent.replace('  ', ' ')\n",
    "            sent = sent.replace('   ', ' ')\n",
    "            sent = sent.replace('    ', ' ')\n",
    "            sent = sent.replace('     ', ' ')\n",
    "            sent = sent.replace('      ', ' ')\n",
    "\n",
    "            # replace breaks\n",
    "            sent = sent.replace(\"break\", \"<break>\")\n",
    "            sent = sent.replace(\"\\n\", \"<break>\")\n",
    "            sent = sent.replace(\"BREAK\", \"<break>\")\n",
    "            \n",
    "            pd_df_list[i][1].loc[j] = sent\n",
    "\n",
    "        pd_df_list[i] = pd_df_list[i].loc[0:len(pd_df_list[i][1])]\n",
    "\n",
    "# group into one pandas dataframe\n",
    "for i in range(0, len(pd_df_list)):\n",
    "    if i == 0:\n",
    "        pd_df = pd_df_list[i]\n",
    "    else:\n",
    "        pd_df = pd.concat([pd_df, pd_df_list[i]])\n",
    "\n",
    "pd_df.columns = ['handle', 'post']\n",
    "\n",
    "pd_df['target'] = pd_df['handle']\n",
    "pd_df['target'][pd_df['handle']=='KimKardashian'] = 0\n",
    "pd_df['target'][pd_df['handle']=='KendallJenner'] = 1\n",
    "pd_df['target'][pd_df['handle']=='KylieJenner'] = 2\n",
    "\n",
    "pd_df[['post', 'target', 'handle']].to_csv(write_data_dir+'/instagram_celeb_attribution.csv')\n",
    "pd_df[['post', 'target', 'handle']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ['acl22-data/raw-data/raw-data-authorship-attribution/KimKardashian-raw-scraped-facebook.csv', 'acl22-data/raw-data/raw-data-authorship-attribution/KendallJenner-raw-scraped-facebook.csv', 'acl22-data/raw-data/raw-data-authorship-attribution/KylieJenner-raw-scraped-facebook.csv'] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                  0                                                  1\n",
      "2   Kim Kardashian                    That‚Äôs what friends are for!!!!\n",
      "14  Kim Kardashian  Our Star Search audition! Clearly we didn‚Äôt ge... \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                  0                                                  1\n",
      "10  Kendall Jenner                  i thrive at this time of the year\n",
      "16  Kendall Jenner  A lil Moon Oral Care to brighten your day üñ§ #m... \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "               0                                                  1\n",
      "11  KylieJenner  KYLIE X NIGHTMARE ON ELM STREET collection lau...\n",
      "22  KylieJenner  KYLIE BABY vegan and clean baby care ‚òÅÔ∏è 9.28 K... \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>target</th>\n",
       "      <th>handle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thank you for all the birthday wishes</td>\n",
       "      <td>1</td>\n",
       "      <td>Kendall Jenner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>as a lover of fashion and having been incredib...</td>\n",
       "      <td>1</td>\n",
       "      <td>Kendall Jenner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>They can steal your recie but the sauce wont t...</td>\n",
       "      <td>0</td>\n",
       "      <td>Kim Kardashian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in full mommy mode this halloween i hoe everyo...</td>\n",
       "      <td>2</td>\n",
       "      <td>KylieJenner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Thank you Adam Luck Kelly Doyle and Larry Morr...</td>\n",
       "      <td>0</td>\n",
       "      <td>Kim Kardashian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 post target          handle\n",
       "4               thank you for all the birthday wishes      1  Kendall Jenner\n",
       "21  as a lover of fashion and having been incredib...      1  Kendall Jenner\n",
       "19  They can steal your recie but the sauce wont t...      0  Kim Kardashian\n",
       "3   in full mommy mode this halloween i hoe everyo...      2     KylieJenner\n",
       "13  Thank you Adam Luck Kelly Doyle and Larry Morr...      0  Kim Kardashian"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "3. authorship attribution facebook\n",
    "\"\"\"\n",
    "\n",
    "#still need to do\n",
    "\n",
    "file_list = glob.glob('acl22-data/raw-data/raw-data-authorship-attribution/*-facebook.csv')\n",
    "len(file_list)\n",
    "print('\\n\\n', file_list, '\\n\\n')\n",
    "pd_df_list = []\n",
    "for f in file_list:\n",
    "    pd_df_list.append(pd.read_csv(f, header=None))\n",
    "\n",
    "print('\\n\\n', pd_df_list[0].sample(2), '\\n\\n')\n",
    "print('\\n\\n', pd_df_list[1].sample(2), '\\n\\n')\n",
    "print('\\n\\n', pd_df_list[2].sample(2), '\\n\\n')\n",
    "\n",
    "# main pipeline\n",
    "for i in range(0, len(pd_df_list)):\n",
    "\n",
    "    # if there are at least 62 lines:\n",
    "    if len(pd_df_list[i][1])>=62:\n",
    "        for j in range(0,61):\n",
    "            string=str(pd_df_list[i][1].loc[j])\n",
    "\n",
    "            # remove emojis \n",
    "            string = remove_emojis(string)\n",
    "\n",
    "            string_list = string.split(' ')\n",
    "            itr = 0\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "\n",
    "                    #replacing <user>\n",
    "                    if word[0] == \"@\":\n",
    "                        string_list[itr] = '@<user>'\n",
    "\n",
    "                    #replacing <url>\n",
    "                    elif word[0:5] == 'https':\n",
    "                        string_list[itr] = '<url>'\n",
    "            \n",
    "                itr+=1\n",
    "\n",
    "            string = \" \".join(string_list)\n",
    "\n",
    "            #replacing invalid chars\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "                    for ii in range(0, len(word)):\n",
    "                        if word[ii] not in chars_valid:\n",
    "                            string = string.replace(word[ii],'')\n",
    "                            \n",
    "            # tokenize\n",
    "            words_tok = word_tokenize(string)\n",
    "            sent = \" \".join(words_tok)\n",
    "            sent = sent.replace('@ < user >', '@<user>')\n",
    "            sent = sent.replace('< url >', '<url>')\n",
    "\n",
    "            # replacing multiple spaces\n",
    "            sent = sent.replace('  ', ' ')\n",
    "            sent = sent.replace('   ', ' ')\n",
    "            sent = sent.replace('    ', ' ')\n",
    "            sent = sent.replace('     ', ' ')\n",
    "            sent = sent.replace('      ', ' ')\n",
    "\n",
    "            # replace breaks\n",
    "            sent = sent.replace(\"break\", \"<break>\")\n",
    "            sent = sent.replace(\"BREAK\", \"<break>\")\n",
    "\n",
    "            \n",
    "            sent = sent.replace(\"\\n\", \"<break>\")\n",
    "            pd_df_list[i][1].loc[j] = sent\n",
    "        pd_df_list[i] = pd_df_list[i].loc[0:61] \n",
    "\n",
    "    # else there are less than 62 lines\n",
    "    else:\n",
    "        for j in range(0,len(pd_df_list[i][1])):\n",
    "            string=str(pd_df_list[i][1].loc[j])\n",
    "\n",
    "            # remove emojis \n",
    "            string = remove_emojis(string)\n",
    "\n",
    "            string_list = string.split(' ')\n",
    "            itr = 0\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "\n",
    "                    #replacing <user>\n",
    "                    if word[0] == \"@\":\n",
    "                        string_list[itr] = '@<user>'\n",
    "\n",
    "                    #replacing <url>\n",
    "                    elif word[0:5] == 'https':\n",
    "                        string_list[itr] = '<url>'\n",
    "            \n",
    "                itr+=1\n",
    "\n",
    "            string = \" \".join(string_list)\n",
    "\n",
    "            #replacing invalid chars\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "                    for ii in range(0, len(word)):\n",
    "                        if word[ii] not in chars_valid:\n",
    "                            string = string.replace(word[ii],'')\n",
    "                            \n",
    "            # tokenize\n",
    "            words_tok = word_tokenize(string)\n",
    "            sent = \" \".join(words_tok)\n",
    "            sent = sent.replace('@ < user >', '@<user>')\n",
    "            sent = sent.replace('< url >', '<url>')\n",
    "\n",
    "            # replacing multiple spaces\n",
    "            sent = sent.replace('  ', ' ')\n",
    "            sent = sent.replace('   ', ' ')\n",
    "            sent = sent.replace('    ', ' ')\n",
    "            sent = sent.replace('     ', ' ')\n",
    "            sent = sent.replace('      ', ' ')\n",
    "\n",
    "            # replace breaks\n",
    "            sent = sent.replace(\"break\", \"<break>\")\n",
    "            sent = sent.replace(\"\\n\", \"<break>\")\n",
    "            sent = sent.replace(\"BREAK\", \"<break>\")\n",
    "            \n",
    "            pd_df_list[i][1].loc[j] = sent\n",
    "\n",
    "        pd_df_list[i] = pd_df_list[i].loc[0:len(pd_df_list[i][1])]\n",
    "\n",
    "# group into one pandas dataframe\n",
    "for i in range(0, len(pd_df_list)):\n",
    "    if i == 0:\n",
    "        pd_df = pd_df_list[i]\n",
    "    else:\n",
    "        pd_df = pd.concat([pd_df, pd_df_list[i]])\n",
    "\n",
    "pd_df.columns = ['handle', 'post']\n",
    "\n",
    "pd_df['target'] = pd_df['handle']\n",
    "pd_df['target'][pd_df['handle']=='Kim Kardashian'] = 0\n",
    "pd_df['target'][pd_df['handle']=='Kendall Jenner'] = 1\n",
    "pd_df['target'][pd_df['handle']=='KylieJenner'] = 2\n",
    "\n",
    "pd_df[['post', 'target', 'handle']].to_csv(write_data_dir+'/facebook_celeb_attribution.csv')\n",
    "pd_df[['post', 'target', 'handle']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9306d298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>target</th>\n",
       "      <th>handle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>NEW @&lt;user&gt; CAMPAIGN BY @&lt;user&gt; @&lt;user&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>KendallJenner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Words cant describe my love for you OMG I love...</td>\n",
       "      <td>0</td>\n",
       "      <td>KimKardashian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Halloween 2021 CowBot Costume by Manfred Thier...</td>\n",
       "      <td>0</td>\n",
       "      <td>KimKardashian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>@&lt;user&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>KylieJenner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Pyro</td>\n",
       "      <td>1</td>\n",
       "      <td>KendallJenner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  post target         handle\n",
       "205            NEW @<user> CAMPAIGN BY @<user> @<user>      1  KendallJenner\n",
       "95   Words cant describe my love for you OMG I love...      0  KimKardashian\n",
       "106  Halloween 2021 CowBot Costume by Manfred Thier...      0  KimKardashian\n",
       "337                                            @<user>      2    KylieJenner\n",
       "189                                               Pyro      1  KendallJenner"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "4. authorship attribution merging IG and Twitter\n",
    "\"\"\"\n",
    "\n",
    "insta_final = pd.read_csv('acl22-data/intermediate-data/cleaned-data-authorship-attribution/'+experiment_name+'/instagram_celeb_attribution.csv')\n",
    "insta_final.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "twitter_final = pd.read_csv('acl22-data/intermediate-data/cleaned-data-authorship-attribution/'+experiment_name+'/twitter_mixing_celeb_attribution.csv')\n",
    "twitter_final.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "mixed_list = []\n",
    "for i in range(0,len(insta_final)):\n",
    "    mixed_list.append(pd.DataFrame(insta_final.loc[i]).T)\n",
    "for i in range(0,len(twitter_final)):\n",
    "    mixed_list.append(pd.DataFrame(twitter_final.loc[i]).T)\n",
    "    \n",
    "\n",
    "mixed = pd.concat(mixed_list)\n",
    "mixed.sort_values('target', axis=0, ascending=True, inplace=True, kind='quicksort', na_position='last', ignore_index=False, key=None)\n",
    "mixed = mixed.reset_index(drop=True)\n",
    "mixed.to_csv(write_data_dir+'/mixed_celeb_attribution.csv')\n",
    "mixed.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authorship profiling internal vars\n",
    "\n",
    "experiment_name = 'v0' #folder name for saving new outputs\n",
    "chars_valid = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\"\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\",\n",
    "               \"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\n",
    "               \"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\",\"@\",\"#\",\"<\",\">\"]\n",
    "\n",
    "write_data_dir = 'acl22-data/intermediate-data/cleaned-data-authorship-profiling/'+experiment_name\n",
    "if not os.path.isdir(write_data_dir):\n",
    "    os.mkdir(write_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " There are 186 twitter accounts for profiling. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "               0                                                  1\n",
      "45  ConanOBrien  I'm worried I offended my Uber driver because ...\n",
      "0   ConanOBrien  Apologies for sneezing during this interview, ... \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Boom It haened Im on @&lt;user&gt; AND IM LIVE FOR A...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Direk @&lt;user&gt; natouch ako sa ost mo Naakasaya ...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>These are BEAUTIFULLY GORGEOUS man &lt;url&gt;&lt;sep&gt;....</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>If I kee going to the farm there wont be any s...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>@&lt;user&gt; Right&lt;sep&gt;. Its cool that Im crying # ...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  post target\n",
       "181  Boom It haened Im on @<user> AND IM LIVE FOR A...      M\n",
       "182  Direk @<user> natouch ako sa ost mo Naakasaya ...      F\n",
       "183  These are BEAUTIFULLY GORGEOUS man <url><sep>....      M\n",
       "184  If I kee going to the farm there wont be any s...      M\n",
       "185  @<user> Right<sep>. Its cool that Im crying # ...      F"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "5. authorship profiling twitter\n",
    "\"\"\"\n",
    "\n",
    "file_list = glob.glob('acl22-data/raw-data/raw-data-authorship-profiling/*-twitter.csv')\n",
    "print('\\n\\n', 'There are', len(file_list), 'twitter accounts for profiling. \\n\\n')\n",
    "pd_df_list = []\n",
    "for f in file_list:\n",
    "    pd_df_list.append(pd.read_csv(f, header=None))\n",
    "\n",
    "print('\\n\\n', pd_df_list[0].sample(2), '\\n\\n')\n",
    "\n",
    "\n",
    "# main pipeline twitter data\n",
    "for i in range(0, len(pd_df_list)):\n",
    "\n",
    "    # if there are at least 62 lines:\n",
    "    if len(pd_df_list[i][1])>=62*2:\n",
    "        for j in range(0,61*2):\n",
    "            string=str(pd_df_list[i][1].loc[j])\n",
    "\n",
    "            # remove emojis \n",
    "            string = remove_emojis(string)\n",
    "\n",
    "            string_list = string.split(' ')\n",
    "            itr = 0\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "\n",
    "                    #replacing <user>\n",
    "                    if word[0] == \"@\":\n",
    "                        string_list[itr] = '@<user>'\n",
    "\n",
    "                    #replacing <url>\n",
    "                    elif word[0:5] == 'https':\n",
    "                        string_list[itr] = '<url>'\n",
    "            \n",
    "                itr+=1\n",
    "\n",
    "            string = \" \".join(string_list)\n",
    "\n",
    "            #replacing invalid chars\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "                    for ii in range(0, len(word)):\n",
    "                        if word[ii] not in chars_valid:\n",
    "                            string = string.replace(word[ii],'')\n",
    "                            \n",
    "            # tokenize\n",
    "            words_tok = word_tokenize(string)\n",
    "            sent = \" \".join(words_tok)\n",
    "            sent = sent.replace('@ < user >', '@<user>')\n",
    "            sent = sent.replace('< url >', '<url>')\n",
    "\n",
    "            # replacing multiple spaces\n",
    "            sent = sent.replace('  ', ' ')\n",
    "            sent = sent.replace('   ', ' ')\n",
    "            sent = sent.replace('    ', ' ')\n",
    "            sent = sent.replace('     ', ' ')\n",
    "            sent = sent.replace('      ', ' ')\n",
    "\n",
    "            # replace breaks\n",
    "            sent = sent.replace(\"break\", \"<break>\")\n",
    "            sent = sent.replace(\"BREAK\", \"<break>\")\n",
    "\n",
    "            \n",
    "            sent = sent.replace(\"\\n\", \"<break>\")\n",
    "            pd_df_list[i][1].loc[j] = sent\n",
    "        pd_df_list[i] = pd_df_list[i].loc[0:61*2] \n",
    "\n",
    "    # else there are less than 62 lines\n",
    "    else:\n",
    "        for j in range(0,len(pd_df_list[i][1])):\n",
    "            string=str(pd_df_list[i][1].loc[j])\n",
    "\n",
    "            # remove emojis \n",
    "            string = remove_emojis(string)\n",
    "\n",
    "            string_list = string.split(' ')\n",
    "            itr = 0\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "\n",
    "                    #replacing <user>\n",
    "                    if word[0] == \"@\":\n",
    "                        string_list[itr] = '@<user>'\n",
    "\n",
    "                    #replacing <url>\n",
    "                    elif word[0:5] == 'https':\n",
    "                        string_list[itr] = '<url>'\n",
    "            \n",
    "                itr+=1\n",
    "\n",
    "            string = \" \".join(string_list)\n",
    "\n",
    "            #replacing invalid chars\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "                    for ii in range(0, len(word)):\n",
    "                        if word[ii] not in chars_valid:\n",
    "                            string = string.replace(word[ii],'')\n",
    "                            \n",
    "            # tokenize\n",
    "            words_tok = word_tokenize(string)\n",
    "            sent = \" \".join(words_tok)\n",
    "            sent = sent.replace('@ < user >', '@<user>')\n",
    "            sent = sent.replace('< url >', '<url>')\n",
    "\n",
    "            # replacing multiple spaces\n",
    "            sent = sent.replace('  ', ' ')\n",
    "            sent = sent.replace('   ', ' ')\n",
    "            sent = sent.replace('    ', ' ')\n",
    "            sent = sent.replace('     ', ' ')\n",
    "            sent = sent.replace('      ', ' ')\n",
    "\n",
    "            # replace breaks\n",
    "            sent = sent.replace(\"break\", \"<break>\")\n",
    "            sent = sent.replace(\"\\n\", \"<break>\")\n",
    "            sent = sent.replace(\"BREAK\", \"<break>\")\n",
    "            \n",
    "            pd_df_list[i][1].loc[j] = sent\n",
    "\n",
    "        pd_df_list[i] = pd_df_list[i].loc[0:len(pd_df_list[i][1])]\n",
    "\n",
    "# group and add <sep>\n",
    "for i in range(0, len(pd_df_list)):\n",
    "    if i == 0:\n",
    "        pd_df = pd_df_list[i].groupby([0])[1].apply('<sep>. '.join).reset_index()\n",
    "    else:\n",
    "        test = pd_df_list[i].groupby([0])[1].apply('<sep>. '.join).reset_index()\n",
    "        pd_df = pd.concat([pd_df, test])\n",
    "\n",
    "pd_df.columns = ['Twitter_handle', 'tweet']\n",
    "\n",
    "sex_df = pd.read_csv('acl22-data/raw-data/raw-data-authorship-profiling/gender_labels.csv')\n",
    "sex_df.columns = ['Gender', 'Twitter_handle', 'Facebook_name', 'Insta_handle', 'Insta Id']\n",
    "total_df = pd.merge(pd_df.assign(Twitter_handle=pd_df['Twitter_handle'].str.lower()), sex_df.assign(Twitter_handle=sex_df['Twitter_handle'].str.lower()), how='left', on='Twitter_handle')\n",
    "\n",
    "total_df = total_df[['tweet', 'Gender']]\n",
    "total_df.columns = ['post', 'target']\n",
    "total_df.to_csv(write_data_dir+'/twitter_celeb_profiling.csv')\n",
    "total_df.tail(5)\n",
    "\n",
    "\n",
    "# pipeline for mixing twitter data (half the size)\n",
    "for i in range(0, len(pd_df_list)):\n",
    "\n",
    "    # if there are at least 62 lines:\n",
    "    if len(pd_df_list[i][1])>=62:\n",
    "        for j in range(0,61):\n",
    "            string=str(pd_df_list[i][1].loc[j])\n",
    "\n",
    "            # remove emojis \n",
    "            string = remove_emojis(string)\n",
    "\n",
    "            string_list = string.split(' ')\n",
    "            itr = 0\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "\n",
    "                    #replacing <user>\n",
    "                    if word[0] == \"@\":\n",
    "                        string_list[itr] = '@<user>'\n",
    "\n",
    "                    #replacing <url>\n",
    "                    elif word[0:5] == 'https':\n",
    "                        string_list[itr] = '<url>'\n",
    "            \n",
    "                itr+=1\n",
    "\n",
    "            string = \" \".join(string_list)\n",
    "\n",
    "            #replacing invalid chars\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "                    for ii in range(0, len(word)):\n",
    "                        if word[ii] not in chars_valid:\n",
    "                            string = string.replace(word[ii],'')\n",
    "                            \n",
    "            # tokenize\n",
    "            words_tok = word_tokenize(string)\n",
    "            sent = \" \".join(words_tok)\n",
    "            sent = sent.replace('@ < user >', '@<user>')\n",
    "            sent = sent.replace('< url >', '<url>')\n",
    "\n",
    "            # replacing multiple spaces\n",
    "            sent = sent.replace('  ', ' ')\n",
    "            sent = sent.replace('   ', ' ')\n",
    "            sent = sent.replace('    ', ' ')\n",
    "            sent = sent.replace('     ', ' ')\n",
    "            sent = sent.replace('      ', ' ')\n",
    "\n",
    "            # replace breaks\n",
    "            sent = sent.replace(\"break\", \"<break>\")\n",
    "            sent = sent.replace(\"BREAK\", \"<break>\")\n",
    "\n",
    "            \n",
    "            sent = sent.replace(\"\\n\", \"<break>\")\n",
    "            pd_df_list[i][1].loc[j] = sent\n",
    "        pd_df_list[i] = pd_df_list[i].loc[0:61] \n",
    "\n",
    "    # else there are less than 62 lines\n",
    "    else:\n",
    "        for j in range(0,len(pd_df_list[i][1])):\n",
    "            string=str(pd_df_list[i][1].loc[j])\n",
    "\n",
    "            # remove emojis \n",
    "            string = remove_emojis(string)\n",
    "\n",
    "            string_list = string.split(' ')\n",
    "            itr = 0\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "\n",
    "                    #replacing <user>\n",
    "                    if word[0] == \"@\":\n",
    "                        string_list[itr] = '@<user>'\n",
    "\n",
    "                    #replacing <url>\n",
    "                    elif word[0:5] == 'https':\n",
    "                        string_list[itr] = '<url>'\n",
    "            \n",
    "                itr+=1\n",
    "\n",
    "            string = \" \".join(string_list)\n",
    "\n",
    "            #replacing invalid chars\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "                    for ii in range(0, len(word)):\n",
    "                        if word[ii] not in chars_valid:\n",
    "                            string = string.replace(word[ii],'')\n",
    "                            \n",
    "            # tokenize\n",
    "            words_tok = word_tokenize(string)\n",
    "            sent = \" \".join(words_tok)\n",
    "            sent = sent.replace('@ < user >', '@<user>')\n",
    "            sent = sent.replace('< url >', '<url>')\n",
    "\n",
    "            # replacing multiple spaces\n",
    "            sent = sent.replace('  ', ' ')\n",
    "            sent = sent.replace('   ', ' ')\n",
    "            sent = sent.replace('    ', ' ')\n",
    "            sent = sent.replace('     ', ' ')\n",
    "            sent = sent.replace('      ', ' ')\n",
    "\n",
    "            # replace breaks\n",
    "            sent = sent.replace(\"break\", \"<break>\")\n",
    "            sent = sent.replace(\"\\n\", \"<break>\")\n",
    "            sent = sent.replace(\"BREAK\", \"<break>\")\n",
    "            \n",
    "            pd_df_list[i][1].loc[j] = sent\n",
    "\n",
    "        pd_df_list[i] = pd_df_list[i].loc[0:len(pd_df_list[i][1])]\n",
    "\n",
    "# group and add <sep>\n",
    "for i in range(0, len(pd_df_list)):\n",
    "    if i == 0:\n",
    "        pd_df = pd_df_list[i].groupby([0])[1].apply('<sep>. '.join).reset_index()\n",
    "    else:\n",
    "        test = pd_df_list[i].groupby([0])[1].apply('<sep>. '.join).reset_index()\n",
    "        pd_df = pd.concat([pd_df, test])\n",
    "\n",
    "pd_df.columns = ['Twitter_handle', 'tweet']\n",
    "\n",
    "sex_df = pd.read_csv('acl22-data/raw-data/raw-data-authorship-profiling/gender_labels.csv')\n",
    "sex_df.columns = ['Gender', 'Twitter_handle', 'Facebook_name', 'Insta_handle', 'Insta Id']\n",
    "total_df = pd.merge(pd_df.assign(Twitter_handle=pd_df['Twitter_handle'].str.lower()), sex_df.assign(Twitter_handle=sex_df['Twitter_handle'].str.lower()), how='left', on='Twitter_handle')\n",
    "\n",
    "total_df = total_df[['tweet', 'Gender']]\n",
    "total_df.columns = ['post', 'target']\n",
    "total_df.to_csv(write_data_dir+'/twitter_mixing_celeb_profiling.csv')\n",
    "total_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e3f5e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " There are 186 twitter accounts for profiling. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "               0                                                  1\n",
      "43  anupampkher  ‚Äú‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï ‡§î‡§∞ ‡§∏‡§°‡§º‡§ï ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§è‡§ï ‡§ú‡•à‡§∏‡•á ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç..BREAK‡§ñ...\n",
      "41  anupampkher  Met my assistant #Dattu after almost 6 months,... \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "             0                                                  1\n",
      "10  liampayne                                   Changing seasons\n",
      "4   liampayne  Loving the new @hugo_official work out gear. H... \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                   0                                                  1\n",
      "30  d_degeaofficial                                                  üí¨\n",
      "41  d_degeaofficial  One of the most special days of the year ‚ù§ @ma... \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Hay birthday to the brightest most beautiful s...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Good God I love my fans so much its crazy I th...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>&lt;break&gt;Head to toe @&lt;user&gt; @&lt;user&gt; &lt;break&gt;&lt;bre...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Hay Saturday&lt;sep&gt;. Hay birthday to this amazin...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>If you dont like what Im saying kee watching g...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  post target\n",
       "181  Hay birthday to the brightest most beautiful s...      M\n",
       "182  Good God I love my fans so much its crazy I th...      F\n",
       "183  <break>Head to toe @<user> @<user> <break><bre...      F\n",
       "184  Hay Saturday<sep>. Hay birthday to this amazin...      F\n",
       "185  If you dont like what Im saying kee watching g...      F"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "6. authorship profiling instagram\n",
    "\"\"\"\n",
    "\n",
    "file_list = glob.glob('acl22-data/raw-data/raw-data-authorship-profiling/*-instagram.csv')\n",
    "len(file_list)\n",
    "print('\\n\\n', 'There are', len(file_list), 'twitter accounts for profiling. \\n\\n')\n",
    "pd_df_list = []\n",
    "for f in file_list:\n",
    "    pd_df_list.append(pd.read_csv(f, header=None))\n",
    "\n",
    "print('\\n\\n', pd_df_list[0].sample(2), '\\n\\n')\n",
    "print('\\n\\n', pd_df_list[1].sample(2), '\\n\\n')\n",
    "print('\\n\\n', pd_df_list[2].sample(2), '\\n\\n')\n",
    "\n",
    "# main pipeline\n",
    "for i in range(0, len(pd_df_list)):\n",
    "\n",
    "    # if there are at least 62 lines:\n",
    "    if len(pd_df_list[i][1])>=62:\n",
    "        for j in range(0,61):\n",
    "            string=str(pd_df_list[i][1].loc[j])\n",
    "\n",
    "            # remove emojis \n",
    "            string = remove_emojis(string)\n",
    "\n",
    "            string_list = string.split(' ')\n",
    "            itr = 0\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "\n",
    "                    #replacing <user>\n",
    "                    if word[0] == \"@\":\n",
    "                        string_list[itr] = '@<user>'\n",
    "\n",
    "                    #replacing <url>\n",
    "                    elif word[0:5] == 'https':\n",
    "                        string_list[itr] = '<url>'\n",
    "            \n",
    "                itr+=1\n",
    "\n",
    "            string = \" \".join(string_list)\n",
    "\n",
    "            #replacing invalid chars\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "                    for ii in range(0, len(word)):\n",
    "                        if word[ii] not in chars_valid:\n",
    "                            string = string.replace(word[ii],'')\n",
    "                            \n",
    "            # tokenize\n",
    "            words_tok = word_tokenize(string)\n",
    "            sent = \" \".join(words_tok)\n",
    "            sent = sent.replace('@ < user >', '@<user>')\n",
    "            sent = sent.replace('< url >', '<url>')\n",
    "\n",
    "            # replacing multiple spaces\n",
    "            sent = sent.replace('  ', ' ')\n",
    "            sent = sent.replace('   ', ' ')\n",
    "            sent = sent.replace('    ', ' ')\n",
    "            sent = sent.replace('     ', ' ')\n",
    "            sent = sent.replace('      ', ' ')\n",
    "\n",
    "            # replace breaks\n",
    "            sent = sent.replace(\"break\", \"<break>\")\n",
    "            sent = sent.replace(\"BREAK\", \"<break>\")\n",
    "\n",
    "            \n",
    "            sent = sent.replace(\"\\n\", \"<break>\")\n",
    "            pd_df_list[i][1].loc[j] = sent\n",
    "        pd_df_list[i] = pd_df_list[i].loc[0:61] \n",
    "\n",
    "    # else there are less than 62 lines\n",
    "    else:\n",
    "        for j in range(0,len(pd_df_list[i][1])):\n",
    "            string=str(pd_df_list[i][1].loc[j])\n",
    "\n",
    "            # remove emojis \n",
    "            string = remove_emojis(string)\n",
    "\n",
    "            string_list = string.split(' ')\n",
    "            itr = 0\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "\n",
    "                    #replacing <user>\n",
    "                    if word[0] == \"@\":\n",
    "                        string_list[itr] = '@<user>'\n",
    "\n",
    "                    #replacing <url>\n",
    "                    elif word[0:5] == 'https':\n",
    "                        string_list[itr] = '<url>'\n",
    "            \n",
    "                itr+=1\n",
    "\n",
    "            string = \" \".join(string_list)\n",
    "\n",
    "            #replacing invalid chars\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "                    for ii in range(0, len(word)):\n",
    "                        if word[ii] not in chars_valid:\n",
    "                            string = string.replace(word[ii],'')\n",
    "                            \n",
    "            # tokenize\n",
    "            words_tok = word_tokenize(string)\n",
    "            sent = \" \".join(words_tok)\n",
    "            sent = sent.replace('@ < user >', '@<user>')\n",
    "            sent = sent.replace('< url >', '<url>')\n",
    "\n",
    "            # replacing multiple spaces\n",
    "            sent = sent.replace('  ', ' ')\n",
    "            sent = sent.replace('   ', ' ')\n",
    "            sent = sent.replace('    ', ' ')\n",
    "            sent = sent.replace('     ', ' ')\n",
    "            sent = sent.replace('      ', ' ')\n",
    "\n",
    "            # replace breaks\n",
    "            sent = sent.replace(\"break\", \"<break>\")\n",
    "            sent = sent.replace(\"\\n\", \"<break>\")\n",
    "            sent = sent.replace(\"BREAK\", \"<break>\")\n",
    "            \n",
    "            pd_df_list[i][1].loc[j] = sent\n",
    "\n",
    "        pd_df_list[i] = pd_df_list[i].loc[0:len(pd_df_list[i][1])]\n",
    "\n",
    "# group and add <sep>\n",
    "for i in range(0, len(pd_df_list)):\n",
    "    if i == 0:\n",
    "        pd_df = pd_df_list[i].groupby([0])[1].apply('<sep>. '.join).reset_index()\n",
    "    else:\n",
    "        test = pd_df_list[i].groupby([0])[1].apply('<sep>. '.join).reset_index()\n",
    "        pd_df = pd.concat([pd_df, test])\n",
    "\n",
    "pd_df.columns = ['Insta_handle', 'IG caption']\n",
    "pd_df = pd_df[pd_df['Insta_handle'] != '0.0']\n",
    "\n",
    "\n",
    "sex_df = pd.read_csv('acl22-data/raw-data/raw-data-authorship-profiling/gender_labels.csv')\n",
    "sex_df.columns = ['Gender', 'Twitter_handle', 'Facebook_name', 'Insta_handle', 'Insta Id']\n",
    "total_df = pd.merge(pd_df.assign(Insta_handle=pd_df['Insta_handle'].str.lower()), sex_df.assign(Insta_handle=sex_df['Insta_handle'].str.lower()), how='left', on='Insta_handle')\n",
    "\n",
    "total_df = total_df[['IG caption', 'Gender']]\n",
    "total_df.columns = ['post', 'target']\n",
    "total_df.to_csv(write_data_dir+'/instagram_celeb_profiling.csv')\n",
    "total_df.tail(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " There are 50 twitter accounts for profiling. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                     0                                                  1\n",
      "14  Leonardo DiCaprio  Protecting 30 percent of the planet in the mos...\n",
      "7   Leonardo DiCaprio  \"We will not be silent. We will never be silen... \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "           0                                                  1\n",
      "8  joejonas  Phoenix was special. Thank you all for a great...\n",
      "9  joejonas  Down to the last 2 shows of the #RememberThisT... \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                  0                                                  1\n",
      "1  Cara Delevingne  TUNE IN NOW for the STANDING IN SOLIDARITY pan...\n",
      "8  Cara Delevingne  Just two hours until I go live with @MyEcoReso... \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>handle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This afternoon I gratefully received my second...</td>\n",
       "      <td>Ivanka Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here are some classic British recies erfect fo...</td>\n",
       "      <td>Gordon Ramsay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good morning hay Monday namaskar The BJP is a ...</td>\n",
       "      <td>Rajdeep Sardesai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Travelling with aa after long time&lt;sep&gt;. At an...</td>\n",
       "      <td>Saina Nehwal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oh na na Myke Towers x me x Tainy out now &lt;url...</td>\n",
       "      <td>Camilo Cabello</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post            handle\n",
       "0  This afternoon I gratefully received my second...      Ivanka Trump\n",
       "0  Here are some classic British recies erfect fo...     Gordon Ramsay\n",
       "0  Good morning hay Monday namaskar The BJP is a ...  Rajdeep Sardesai\n",
       "0  Travelling with aa after long time<sep>. At an...      Saina Nehwal\n",
       "0  Oh na na Myke Towers x me x Tainy out now <url...    Camilo Cabello"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "7. authorship profiling facebook\n",
    "\"\"\"\n",
    "\n",
    "#still need to do\n",
    "\n",
    "file_list = glob.glob('acl22-data/raw-data/raw-data-authorship-profiling/*-facebook.csv')\n",
    "len(file_list)\n",
    "print('\\n\\n', 'There are', len(file_list), 'twitter accounts for profiling. \\n\\n')\n",
    "pd_df_list = []\n",
    "for f in file_list:\n",
    "    pd_df_list.append(pd.read_csv(f, header=None))\n",
    "\n",
    "print('\\n\\n', pd_df_list[0].sample(2), '\\n\\n')\n",
    "print('\\n\\n', pd_df_list[1].sample(2), '\\n\\n')\n",
    "print('\\n\\n', pd_df_list[2].sample(2), '\\n\\n')\n",
    "\n",
    "# main pipeline\n",
    "for i in range(0, len(pd_df_list)):\n",
    "\n",
    "    # if there are at least 62 lines:\n",
    "    if len(pd_df_list[i][1])>=62:\n",
    "        for j in range(0,61):\n",
    "            string=str(pd_df_list[i][1].loc[j])\n",
    "\n",
    "            # remove emojis \n",
    "            string = remove_emojis(string)\n",
    "\n",
    "            string_list = string.split(' ')\n",
    "            itr = 0\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "\n",
    "                    #replacing <user>\n",
    "                    if word[0] == \"@\":\n",
    "                        string_list[itr] = '@<user>'\n",
    "\n",
    "                    #replacing <url>\n",
    "                    elif word[0:5] == 'https':\n",
    "                        string_list[itr] = '<url>'\n",
    "            \n",
    "                itr+=1\n",
    "\n",
    "            string = \" \".join(string_list)\n",
    "\n",
    "            #replacing invalid chars\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "                    for ii in range(0, len(word)):\n",
    "                        if word[ii] not in chars_valid:\n",
    "                            string = string.replace(word[ii],'')\n",
    "                            \n",
    "            # tokenize\n",
    "            words_tok = word_tokenize(string)\n",
    "            sent = \" \".join(words_tok)\n",
    "            sent = sent.replace('@ < user >', '@<user>')\n",
    "            sent = sent.replace('< url >', '<url>')\n",
    "\n",
    "            # replacing multiple spaces\n",
    "            sent = sent.replace('  ', ' ')\n",
    "            sent = sent.replace('   ', ' ')\n",
    "            sent = sent.replace('    ', ' ')\n",
    "            sent = sent.replace('     ', ' ')\n",
    "            sent = sent.replace('      ', ' ')\n",
    "\n",
    "            # replace breaks\n",
    "            sent = sent.replace(\"break\", \"<break>\")\n",
    "            sent = sent.replace(\"BREAK\", \"<break>\")\n",
    "\n",
    "            \n",
    "            sent = sent.replace(\"\\n\", \"<break>\")\n",
    "            pd_df_list[i][1].loc[j] = sent\n",
    "        pd_df_list[i] = pd_df_list[i].loc[0:61] \n",
    "\n",
    "    # else there are less than 62 lines\n",
    "    else:\n",
    "        for j in range(0,len(pd_df_list[i][1])):\n",
    "            string=str(pd_df_list[i][1].loc[j])\n",
    "\n",
    "            # remove emojis \n",
    "            string = remove_emojis(string)\n",
    "\n",
    "            string_list = string.split(' ')\n",
    "            itr = 0\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "\n",
    "                    #replacing <user>\n",
    "                    if word[0] == \"@\":\n",
    "                        string_list[itr] = '@<user>'\n",
    "\n",
    "                    #replacing <url>\n",
    "                    elif word[0:5] == 'https':\n",
    "                        string_list[itr] = '<url>'\n",
    "            \n",
    "                itr+=1\n",
    "\n",
    "            string = \" \".join(string_list)\n",
    "\n",
    "            #replacing invalid chars\n",
    "            for word in string_list:\n",
    "                if len(word) > 0:\n",
    "                    for ii in range(0, len(word)):\n",
    "                        if word[ii] not in chars_valid:\n",
    "                            string = string.replace(word[ii],'')\n",
    "                            \n",
    "            # tokenize\n",
    "            words_tok = word_tokenize(string)\n",
    "            sent = \" \".join(words_tok)\n",
    "            sent = sent.replace('@ < user >', '@<user>')\n",
    "            sent = sent.replace('< url >', '<url>')\n",
    "\n",
    "            # replacing multiple spaces\n",
    "            sent = sent.replace('  ', ' ')\n",
    "            sent = sent.replace('   ', ' ')\n",
    "            sent = sent.replace('    ', ' ')\n",
    "            sent = sent.replace('     ', ' ')\n",
    "            sent = sent.replace('      ', ' ')\n",
    "\n",
    "            # replace breaks\n",
    "            sent = sent.replace(\"break\", \"<break>\")\n",
    "            sent = sent.replace(\"\\n\", \"<break>\")\n",
    "            sent = sent.replace(\"BREAK\", \"<break>\")\n",
    "            \n",
    "            pd_df_list[i][1].loc[j] = sent\n",
    "\n",
    "        pd_df_list[i] = pd_df_list[i].loc[0:len(pd_df_list[i][1])]\n",
    "\n",
    "# group and add <sep>\n",
    "for i in range(0, len(pd_df_list)):\n",
    "    if i == 0:\n",
    "        pd_df = pd_df_list[i].groupby([0])[1].apply('<sep>. '.join).reset_index()\n",
    "    else:\n",
    "        test = pd_df_list[i].groupby([0])[1].apply('<sep>. '.join).reset_index()\n",
    "        pd_df = pd.concat([pd_df, test])\n",
    "\n",
    "pd_df.columns = ['handle', 'post']\n",
    "pd_df = pd_df[['post', 'handle']]\n",
    "pd_df.to_csv(write_data_dir+'/facebook_celeb_profiling.csv')\n",
    "pd_df.tail(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>Mi es tu casa&lt;sep&gt;. Puerto Rico acaba de ganar...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>If we ut the # Gosel at the centre and bear wi...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>So excited that our book suorting @&lt;user&gt; is n...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>&lt;url&gt;&lt;sep&gt;. Back on set # RamSetu &lt;url&gt;&lt;sep&gt;. ...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>Circles January 17&lt;sep&gt;. New York Mag Link in ...</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  post target\n",
       "307  Mi es tu casa<sep>. Puerto Rico acaba de ganar...      M\n",
       "158  If we ut the # Gosel at the centre and bear wi...      M\n",
       "90   So excited that our book suorting @<user> is n...      F\n",
       "65   <url><sep>. Back on set # RamSetu <url><sep>. ...      F\n",
       "351  Circles January 17<sep>. New York Mag Link in ...      M"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "8. authorship profiling merging IG and Twitter\n",
    "\"\"\"\n",
    "\n",
    "insta_final = pd.read_csv('acl22-data/intermediate-data/cleaned-data-authorship-profiling/'+experiment_name+'/instagram_celeb_profiling.csv')\n",
    "insta_final.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "twitter_final = pd.read_csv('acl22-data/intermediate-data/cleaned-data-authorship-profiling/'+experiment_name+'/twitter_mixing_celeb_profiling.csv')\n",
    "twitter_final.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "mixed_list = []\n",
    "for i in range(0,len(insta_final)):\n",
    "    mixed_list.append(pd.DataFrame(insta_final.loc[i]).T)\n",
    "for i in range(0,len(twitter_final)):\n",
    "    mixed_list.append(pd.DataFrame(twitter_final.loc[i]).T)\n",
    "\n",
    "mixed = pd.concat(mixed_list)\n",
    "mixed.sort_values('target', axis=0, ascending=True, inplace=True, kind='quicksort', na_position='last', ignore_index=False, key=None)\n",
    "mixed = mixed.reset_index(drop=True)\n",
    "mixed.to_csv(write_data_dir+'/mixed_celeb_profiling.csv')\n",
    "mixed.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4222c62deee789526b2e6b735eb2f20b12feb76cd92f9c10ded5d55a4a9b08d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('anlp': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
